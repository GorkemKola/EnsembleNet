{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gorkemkola/Desktop/Projects/EnsembleNet/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gorkemkola/Desktop/Projects/EnsembleNet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/gorkemkola/Desktop/Projects/EnsembleNet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ensemblenet.entity import TrainingConfig\n",
    "from ensemblenet.constants import *\n",
    "from ensemblenet.utils import read_yaml, create_directories, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH,\n",
    "        ) -> None:\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        # --- Model Selection and Validation ---\n",
    "        # Add 'attention_ensemble' to your allowed list if that's the name you use\n",
    "        allowed_models = [\n",
    "            \"resnet\", \n",
    "            \"inception_v3\", \n",
    "            \"vit\", \n",
    "            \"attention_ensemble\", \n",
    "            \"squeezenet\", \n",
    "            \"shufflenet\", \n",
    "            \"mobilenet\", \n",
    "            \"attention_ensemble_better\",\n",
    "            \"mnasnet\",\n",
    "            \"ensemblenet\",\n",
    "            \"ensemblenet0\",\n",
    "            \"ensemblenet1\",\n",
    "            \"ensemblenet2\"\n",
    "\n",
    "        ]\n",
    "        if self.params.MODEL_NAME not in allowed_models:\n",
    "             raise ValueError(f\"Invalid MODEL_NAME '{self.params.MODEL_NAME}' in params.yaml. Choose from {allowed_models}\")\n",
    "        self.model_name = self.params.MODEL_NAME\n",
    "        # --- End ---\n",
    "\n",
    "        create_directories(\n",
    "            [\n",
    "                self.config.artifacts_root,\n",
    "                Path(self.config.training.root_dir),\n",
    "                Path(self.config.training.tensorboard_log_dir),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _validate_data_dirs(self, train_dir, val_dir, test_dir, expected_classes):\n",
    "        \"\"\"Check if data directories exist and validate class count.\"\"\"\n",
    "        paths_to_check = {\"train\": train_dir, \"validation\": val_dir, \"test\": test_dir}\n",
    "        for split, path in paths_to_check.items():\n",
    "            if not os.path.isdir(path):\n",
    "                raise NotADirectoryError(f\"{split.capitalize()} data directory not found at: {path}\")\n",
    "            try:\n",
    "                # Count subdirectories (classes) in the training dir for validation\n",
    "                if split == 'train':\n",
    "                    actual_classes = len([name for name in os.listdir(path) if os.path.isdir(os.path.join(path, name))])\n",
    "                    if actual_classes == 0:\n",
    "                         raise ValueError(f\"No subdirectories (classes) found in {path}\")\n",
    "                    if actual_classes != expected_classes:\n",
    "                         logger.warning(f\"Class count mismatch in {split} dir! params.yaml says {expected_classes}, \"\n",
    "                                       f\"but found {actual_classes} subdirectories in {path}. Using {expected_classes} from params.\")\n",
    "                         # Decide if this should be a hard error:\n",
    "                         # raise ValueError(f\"Class count mismatch...\")\n",
    "                    else:\n",
    "                         logger.info(f\"Validated {actual_classes} classes in {path}, matching params.yaml.\")\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Could not validate directory structure in {path}: {e}\")\n",
    "                 raise\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        config_train = self.config.training\n",
    "        config_data = self.config.data_ingestion\n",
    "        config_ensemble = self.config.prepare_ensemble_model\n",
    "        params = self.params\n",
    "\n",
    "        # --- Get Data Paths ---\n",
    "        train_data_path = Path(config_data.train_dir)\n",
    "        valid_data_path = Path(config_data.val_dir)\n",
    "        test_data_path = Path(config_data.test_dir)\n",
    "        # --- End ---\n",
    "\n",
    "        # --- Get Classes and Pretrained Flag ---\n",
    "        num_classes = params.CLASSES\n",
    "        # Determine if pretrained weights should be used based on param\n",
    "        use_pretrained = params.PRETRAINED_WEIGHTS is not None and params.PRETRAINED_WEIGHTS.upper() != 'NONE'\n",
    "        logger.info(f\"Number of classes set to: {num_classes}\")\n",
    "        logger.info(f\"Use pretrained weights: {use_pretrained} (Based on PRETRAINED_WEIGHTS: {params.PRETRAINED_WEIGHTS})\")\n",
    "        # --- End ---\n",
    "\n",
    "        # --- Validate Data Directories ---\n",
    "        self._validate_data_dirs(train_data_path, valid_data_path, test_data_path, num_classes)\n",
    "        # --- End ---\n",
    "\n",
    "        # --- Get Ensemble Model Path ---\n",
    "        ensemble_model_path = Path(config_ensemble.ensemble_model_path)\n",
    "        if self.model_name == \"attention_ensemble\" and not ensemble_model_path.exists():\n",
    "             logger.warning(f\"Selected MODEL_NAME is 'attention_ensemble', but the model file \"\n",
    "                            f\"at {ensemble_model_path} does not exist!\")\n",
    "\n",
    "        # --- Construct model-specific save paths using templates ---\n",
    "        last_model_path = Path(config_train.last_model_path_template.format(model_name=self.model_name))\n",
    "        best_model_path = Path(config_train.best_model_path_template.format(model_name=self.model_name))\n",
    "        create_directories([last_model_path.parent, best_model_path.parent]) # Ensure parent dirs exist\n",
    "        # --- End ---\n",
    "\n",
    "        # --- Log Image Size Warning (as before) ---\n",
    "        expected_size = None\n",
    "        if self.model_name == \"inception_v3\": expected_size = (299, 299)\n",
    "        elif self.model_name == \"vit\": expected_size = (224, 224)\n",
    "        current_size = tuple(params.IMAGE_SIZE[:2])\n",
    "        if expected_size and current_size != expected_size:\n",
    "            logger.warning(f\"Model '{self.model_name}' typically expects input size {expected_size}, \"\n",
    "                           f\"but IMAGE_SIZE is set to {current_size}.\")\n",
    "        # --- End Log ---\n",
    "\n",
    "        find_lr_flag = getattr(params, 'FIND_LR', False) # Default to False if not in params\n",
    "        logger.info(f\"Run Learning Rate Finder: {find_lr_flag}\")\n",
    "        # --- Construct LR finder plot path ---\n",
    "        lr_plot_filename = f\"lr_finder_plot_{self.model_name}.png\"\n",
    "        lr_finder_plot_path = Path(config_train.root_dir) / lr_plot_filename\n",
    "        create_directories([lr_finder_plot_path.parent]) # Ensure parent dir exists\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(config_train.root_dir),\n",
    "            last_model_path=last_model_path,\n",
    "            best_model_path=best_model_path,\n",
    "            train_data_path=train_data_path,\n",
    "            valid_data_path=valid_data_path,\n",
    "            test_data_path=test_data_path,\n",
    "            attention_ensemble_model_path=ensemble_model_path,\n",
    "            model_name=self.model_name,\n",
    "            num_classes=num_classes,         \n",
    "            use_pretrained=use_pretrained,   \n",
    "            lr_finder_plot_path=lr_finder_plot_path,    \n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_early_stopping_patience=params.EARLY_STOPPING_PATIENCE,\n",
    "            params_learning_rate=params.LEARNING_RATE,\n",
    "            params_random_state=params.RANDOM_STATE,\n",
    "            params_find_lr=find_lr_flag,                    \n",
    "        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 13:34:25,594: INFO: utils: yaml file config/config.yaml loaded successfully:]\n",
      "[2025-06-21 13:34:25,597: INFO: utils: yaml file config/params.yaml loaded successfully:]\n",
      "[2025-06-21 13:34:25,598: INFO: utils: created directory at: artifacts:]\n",
      "[2025-06-21 13:34:25,599: INFO: utils: created directory at: artifacts/training:]\n",
      "[2025-06-21 13:34:25,600: INFO: utils: created directory at: artifacts/training/logs:]\n",
      "[2025-06-21 13:34:25,600: INFO: 2615980527: Number of classes set to: 257:]\n",
      "[2025-06-21 13:34:25,600: INFO: 2615980527: Use pretrained weights: True (Based on PRETRAINED_WEIGHTS: IMAGENET1K_V1):]\n",
      "[2025-06-21 13:34:25,601: ERROR: 2615980527: Could not validate directory structure in artifacts/data_ingestion/train: No subdirectories (classes) found in artifacts/data_ingestion/train:]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No subdirectories (classes) found in artifacts/data_ingestion/train",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mConfigurationManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_training_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mConfigurationManager.get_training_config\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     82\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUse pretrained weights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_pretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Based on PRETRAINED_WEIGHTS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams.PRETRAINED_WEIGHTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# --- End ---\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# --- Validate Data Directories ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_data_dirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# --- End ---\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# --- Get Ensemble Model Path ---\u001b[39;00m\n\u001b[32m     90\u001b[39m ensemble_model_path = Path(config_ensemble.ensemble_model_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mConfigurationManager._validate_data_dirs\u001b[39m\u001b[34m(self, train_dir, val_dir, test_dir, expected_classes)\u001b[39m\n\u001b[32m     51\u001b[39m actual_classes = \u001b[38;5;28mlen\u001b[39m([name \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m os.listdir(path) \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(os.path.join(path, name))])\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m actual_classes == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo subdirectories (classes) found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m actual_classes != expected_classes:\n\u001b[32m     55\u001b[39m      logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClass count mismatch in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dir! params.yaml says \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     56\u001b[39m                    \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m subdirectories in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from params.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No subdirectories (classes) found in artifacts/data_ingestion/train"
     ]
    }
   ],
   "source": [
    "ConfigurationManager().get_training_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemblenet.components.ensemblenet import EnsembleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from ignite.engine import Engine\n",
    "from ignite.handlers import FastaiLRFinder\n",
    "import os\n",
    "\n",
    "torch.manual_seed(24120200)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing class imbalance.\n",
    "    \n",
    "    Focal Loss = -alpha * (1 - p_t)^gamma * log(p_t)\n",
    "    \n",
    "    Args:\n",
    "        alpha (float or Tensor): Weighting factor (default: 1.0)\n",
    "        gamma (float): Focusing parameter (default: 2.0)\n",
    "        reduction (str): 'mean', 'sum', or 'none' (default: 'mean')\n",
    "        ignore_index (int): Index to ignore (default: -100)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean', ignore_index=-100):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='none', ignore_index=ignore_index)\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            outputs (Tensor): Model predictions (logits) of shape (N, C)\n",
    "            targets (Tensor): Ground truth labels of shape (N,)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Focal loss\n",
    "        \"\"\"\n",
    "        ce_loss = self.ce_loss(outputs, targets)\n",
    "        p_t = torch.exp(-ce_loss)  # Convert back to probabilities\n",
    "        \n",
    "        # Handle ignore_index\n",
    "        if self.ignore_index >= 0:\n",
    "            ignore_mask = targets == self.ignore_index\n",
    "            p_t = p_t.masked_fill(ignore_mask, 1.0)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        if isinstance(self.alpha, (float, int)):\n",
    "            alpha_weight = self.alpha\n",
    "        else:\n",
    "            alpha_weight = self.alpha.gather(0, targets)\n",
    "        \n",
    "        focal_loss = alpha_weight * focal_weight * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "        self.model = None\n",
    "        self.train_loader = None\n",
    "        self.valid_loader = None\n",
    "        self.test_loader = None\n",
    "        \n",
    "        # Initialize CSV logging\n",
    "        self.setup_csv_logging()\n",
    "        \n",
    "    def setup_csv_logging(self):\n",
    "        \"\"\"Setup CSV files for logging metrics and hyperparameters\"\"\"\n",
    "        # Create logging directory if it doesn't exist\n",
    "        log_dir = Path(self.config.root_dir) / \"logs\"\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Generate timestamp for unique file names\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Define CSV file paths\n",
    "        self.metrics_csv_path = log_dir / f\"training_metrics_{self.config.model_name}.csv\"\n",
    "        self.hyperparams_csv_path = log_dir / f\"hyperparameters_{self.config.model_name}.csv\"\n",
    "        self.lr_finder_csv_path = log_dir / f\"lr_finder_results_{self.config.model_name}.csv\"\n",
    "        \n",
    "        # Initialize metrics CSV with headers\n",
    "        metrics_headers = [\n",
    "            'epoch', 'train_loss', 'valid_loss', 'accuracy', 'precision', \n",
    "            'recall', 'f1_score', 'learning_rate', 'is_best_model', \n",
    "            'early_stopping_counter', 'timestamp'\n",
    "        ]\n",
    "        \n",
    "        with open(self.metrics_csv_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(metrics_headers)\n",
    "        \n",
    "        logger.info(f\"Metrics CSV initialized: {self.metrics_csv_path}\")\n",
    "        \n",
    "        # Save hyperparameters immediately\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def save_hyperparameters(self):\n",
    "        \"\"\"Save all hyperparameters and configuration to CSV\"\"\"\n",
    "        hyperparams_data = []\n",
    "        \n",
    "        # Get all config attributes\n",
    "        config_dict = vars(self.config)\n",
    "        \n",
    "        # Add system information\n",
    "        hyperparams_data.extend([\n",
    "            ('timestamp', datetime.now().isoformat()),\n",
    "            ('device', str(self.device)),\n",
    "            ('torch_version', torch.__version__),\n",
    "            ('cuda_available', torch.cuda.is_available()),\n",
    "            ('cuda_version', torch.version.cuda if torch.cuda.is_available() else 'N/A'),\n",
    "        ])\n",
    "        \n",
    "        # Add configuration parameters\n",
    "        for key, value in config_dict.items():\n",
    "            hyperparams_data.append((key, str(value)))\n",
    "        \n",
    "        # Add derived parameters\n",
    "        if hasattr(self, 'train_loader') and self.train_loader:\n",
    "            hyperparams_data.extend([\n",
    "                ('total_train_samples', len(self.train_loader.dataset)),\n",
    "                ('train_batches_per_epoch', len(self.train_loader)),\n",
    "            ])\n",
    "        \n",
    "        if hasattr(self, 'valid_loader') and self.valid_loader:\n",
    "            hyperparams_data.extend([\n",
    "                ('total_valid_samples', len(self.valid_loader.dataset)),\n",
    "                ('valid_batches_per_epoch', len(self.valid_loader)),\n",
    "            ])\n",
    "            \n",
    "        if hasattr(self, 'test_loader') and self.test_loader:\n",
    "            hyperparams_data.extend([\n",
    "                ('total_test_samples', len(self.test_loader.dataset)),\n",
    "                ('test_batches_per_epoch', len(self.test_loader)),\n",
    "            ])\n",
    "        \n",
    "        # Save to CSV\n",
    "        df_hyperparams = pd.DataFrame(hyperparams_data, columns=['parameter', 'value'])\n",
    "        df_hyperparams.to_csv(self.hyperparams_csv_path, index=False)\n",
    "        \n",
    "        logger.info(f\"Hyperparameters saved to: {self.hyperparams_csv_path}\")\n",
    "        \n",
    "    def log_metrics_to_csv(self, epoch, train_loss, valid_loss, accuracy, precision, \n",
    "                          recall, f1, learning_rate, is_best_model, early_stopping_counter):\n",
    "        \"\"\"Log training metrics to CSV\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        metrics_row = [\n",
    "            epoch + 1,  # 1-indexed for readability\n",
    "            train_loss,\n",
    "            valid_loss,\n",
    "            accuracy,\n",
    "            precision,\n",
    "            recall,\n",
    "            f1,\n",
    "            learning_rate,\n",
    "            is_best_model,\n",
    "            early_stopping_counter,\n",
    "            timestamp\n",
    "        ]\n",
    "        \n",
    "        with open(self.metrics_csv_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(metrics_row)\n",
    "            \n",
    "    def save_lr_finder_results(self, lr_finder, suggested_lr=None):\n",
    "        \"\"\"Save LR finder results to CSV, including suggested learning rate\"\"\"\n",
    "        try:\n",
    "            results = lr_finder.get_results()\n",
    "            if results and 'lr' in results and 'loss' in results:\n",
    "                df_lr = pd.DataFrame({\n",
    "                    'learning_rate': results['lr'],\n",
    "                    'loss': results['loss']\n",
    "                })\n",
    "                df_lr.to_csv(self.lr_finder_csv_path, index=False)\n",
    "                logger.info(f\"LR Finder results saved to: {self.lr_finder_csv_path}\")\n",
    "                \n",
    "                # Save suggested learning rate to a separate file\n",
    "                if suggested_lr is not None:\n",
    "                    # Handle both Path objects and strings\n",
    "                    if hasattr(self.lr_finder_csv_path, 'parent'):\n",
    "                        # Path object\n",
    "                        lr_suggestion_path = self.lr_finder_csv_path.parent / (self.lr_finder_csv_path.stem + '_suggested_lr.txt')\n",
    "                    else:\n",
    "                        # String path\n",
    "                        lr_suggestion_path = str(self.lr_finder_csv_path).replace('.csv', '_suggested_lr.txt')\n",
    "                    \n",
    "                    with open(lr_suggestion_path, 'w') as f:\n",
    "                        f.write(str(suggested_lr))\n",
    "                    logger.info(f\"Suggested learning rate ({suggested_lr:.2e}) saved to: {lr_suggestion_path}\")\n",
    "            else:\n",
    "                logger.warning(\"No LR Finder results to save\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving LR Finder results: {e}\")\n",
    "\n",
    "    def load_suggested_lr(self):\n",
    "        \"\"\"Load previously saved suggested learning rate\"\"\"\n",
    "        try:\n",
    "            # Handle both Path objects and strings\n",
    "            if hasattr(self.lr_finder_csv_path, 'parent'):\n",
    "                # Path object\n",
    "                lr_suggestion_path = self.lr_finder_csv_path.parent / (self.lr_finder_csv_path.stem + '_suggested_lr.txt')\n",
    "            else:\n",
    "                # String path\n",
    "                lr_suggestion_path = str(self.lr_finder_csv_path).replace('.csv', '_suggested_lr.txt')\n",
    "            \n",
    "            if os.path.exists(lr_suggestion_path):\n",
    "                with open(lr_suggestion_path, 'r') as f:\n",
    "                    suggested_lr = float(f.read().strip())\n",
    "                logger.info(f\"Loaded previously suggested learning rate: {suggested_lr:.2e}\")\n",
    "                return suggested_lr\n",
    "            else:\n",
    "                logger.info(\"No previously saved suggested learning rate found\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading suggested learning rate: {e}\")\n",
    "            return None\n",
    "\n",
    "    def find_optimal_lr_ignite(self, start_lr, end_lr=10, num_iter=100, step_mode=\"exp\", force_recalculate=False):\n",
    "        \"\"\"\n",
    "        Runs the LR Range Test using ignite.handlers.FastaiLRFinder.\n",
    "        Returns a suggested learning rate and saves results to CSV.\n",
    "        \n",
    "        Args:\n",
    "            end_lr: Maximum learning rate to test\n",
    "            num_iter: Number of iterations\n",
    "            step_mode: Step mode for learning rate schedule\n",
    "            force_recalculate: If True, recalculates even if saved results exist\n",
    "        \"\"\"\n",
    "        if not self.model or not self.train_loader:\n",
    "            logger.error(\"Model and Train Loader must be initialized before running LR Finder.\")\n",
    "            return None\n",
    "\n",
    "        # Check if we have a previously saved suggested learning rate\n",
    "        if not force_recalculate:\n",
    "            saved_lr = self.load_suggested_lr()\n",
    "            if saved_lr is not None:\n",
    "                logger.info(f\"Using previously calculated suggested learning rate: {saved_lr:.2e}\")\n",
    "                logger.info(\"Set force_recalculate=True to recalculate from scratch\")\n",
    "                return saved_lr\n",
    "\n",
    "        logger.info(f\"--- Running Learning Rate Finder (Ignite) ---\")\n",
    "        logger.info(f\"Using end_lr={end_lr}, num_iter={num_iter}, step_mode='{step_mode}'\")\n",
    "\n",
    "        self.model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        temp_optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=1e-7)\n",
    "\n",
    "        def update_fn(engine, batch):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "            temp_optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            if self.config.model_name == \"attention_ensemble\":\n",
    "                logits = outputs\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "            elif isinstance(outputs, tuple) and hasattr(outputs, 'logits') and hasattr(outputs, 'aux_logits') and self.config.model_name == \"inception_v3\":\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "            elif isinstance(outputs, tuple):\n",
    "                loss = criterion(outputs[0], labels)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            temp_optimizer.step()\n",
    "            return loss.item()\n",
    "\n",
    "        finder_engine = Engine(update_fn)\n",
    "        lr_finder = FastaiLRFinder()\n",
    "        to_save = {\"model\": self.model, \"optimizer\": temp_optimizer}\n",
    "\n",
    "        suggested_lr_final = None\n",
    "        try:\n",
    "            with lr_finder.attach(finder_engine, to_save=to_save, start_lr=start_lr, end_lr=end_lr) as trainer_with_lr_finder:\n",
    "                trainer_with_lr_finder.run(self.train_loader, max_epochs=1)\n",
    "\n",
    "            suggested_lr = lr_finder.lr_suggestion()\n",
    "\n",
    "            logger.info(\"Plotting LR Finder results...\")\n",
    "            fig = lr_finder.plot()\n",
    "            if fig:\n",
    "                plot_path = self.config.lr_finder_plot_path\n",
    "                logger.info(f\"LR Finder plot saved to: {plot_path}\")\n",
    "            else:\n",
    "                logger.warning(\"Could not generate LR Finder plot.\")\n",
    "\n",
    "            if suggested_lr is None:\n",
    "                logger.warning(\"Ignite LR Finder could not suggest a learning rate. Using fallback calculation.\")\n",
    "                results = lr_finder.get_results()\n",
    "                if results and 'lr' in results and 'loss' in results and len(results['loss']) > 0:\n",
    "                    min_loss_idx = results['loss'].index(min(results['loss']))\n",
    "                    min_loss_lr = results['lr'][min_loss_idx]\n",
    "                    suggested_lr_final = min_loss_lr / 10.0\n",
    "                    logger.info(f\"LR at Minimum Loss: {min_loss_lr:.2e}\")\n",
    "                    logger.info(f\"Fallback suggestion (Min Loss / 10): {suggested_lr_final:.2e}\")\n",
    "                else:\n",
    "                    logger.warning(\"Could not retrieve results for fallback calculation.\")\n",
    "                    suggested_lr_final = None\n",
    "            else:\n",
    "                logger.info(f\"Ignite LR Finder suggested learning rate: {suggested_lr:.2e}\")\n",
    "                suggested_lr_final = suggested_lr\n",
    "\n",
    "            # Apply bounds checking\n",
    "            if suggested_lr_final is not None:\n",
    "                if suggested_lr_final > 0.1:\n",
    "                    logger.warning(f\"Suggested LR {suggested_lr_final:.2e} seems high. Clamping to 0.1. Please check the plot!\")\n",
    "                    suggested_lr_final = 0.1\n",
    "                elif suggested_lr_final < 1e-7:\n",
    "                    logger.warning(f\"Suggested LR {suggested_lr_final:.2e} seems too low. Clamping to 1e-7. Please check the plot!\")\n",
    "                    suggested_lr_final = 1e-7\n",
    "\n",
    "            # Save results including the suggested learning rate\n",
    "            self.save_lr_finder_results(lr_finder, suggested_lr_final)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during Ignite LR Finder execution: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            suggested_lr_final = None\n",
    "\n",
    "        logger.info(\"--- Learning Rate Finder (Ignite) Finished ---\")\n",
    "        return suggested_lr_final\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Builds/loads the specified model, using config flags.\"\"\"\n",
    "        logger.info(f\"Building model: {self.config.model_name}\")\n",
    "        num_classes = self.config.num_classes\n",
    "        use_pretrained = self.config.use_pretrained\n",
    "\n",
    "        if self.config.model_name == \"resnet\":\n",
    "            weights = models.ResNet50_Weights.IMAGENET1K_V1 if use_pretrained else None\n",
    "            model = models.resnet50(weights=weights)\n",
    "            log_msg = f\"Loading ResNet50 with {'ImageNet pretrained' if use_pretrained else 'random'} weights.\"\n",
    "            logger.info(log_msg)\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "            logger.info(f\"Adapted ResNet FC layer for {num_classes} classes.\")\n",
    "\n",
    "        elif self.config.model_name == \"inception_v3\":\n",
    "            weights = models.Inception_V3_Weights.IMAGENET1K_V1 if use_pretrained else None\n",
    "            model = models.inception_v3(weights=weights, aux_logits=True, transform_input=True)\n",
    "            log_msg = f\"Loading Inception V3 with {'ImageNet pretrained' if use_pretrained else 'random'} weights.\"\n",
    "            logger.info(log_msg)\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "            logger.info(f\"Adapted Inception V3 main FC layer for {num_classes} classes.\")\n",
    "            if hasattr(model, 'AuxLogits') and model.AuxLogits is not None:\n",
    "                num_ftrs_aux = model.AuxLogits.fc.in_features\n",
    "                model.AuxLogits.fc = nn.Linear(num_ftrs_aux, num_classes)\n",
    "                logger.info(f\"Adapted Inception V3 auxiliary FC layer for {num_classes} classes.\")\n",
    "            else:\n",
    "                logger.info(\"Inception V3 AuxLogits not present or adapted.\")\n",
    "\n",
    "        elif self.config.model_name == \"vit\":\n",
    "            weights = models.ViT_B_16_Weights.IMAGENET1K_V1 if use_pretrained else None\n",
    "            model = models.vit_b_16(weights=weights)\n",
    "            log_msg = f\"Loading ViT-B/16 with {'ImageNet pretrained' if use_pretrained else 'random'} weights.\"\n",
    "            logger.info(log_msg)\n",
    "            num_ftrs = model.heads.head.in_features\n",
    "            model.heads.head = nn.Linear(num_ftrs, num_classes)\n",
    "            logger.info(f\"Adapted ViT classification head for {num_classes} classes.\")\n",
    "\n",
    "        elif self.config.model_name == \"attention_ensemble\":\n",
    "            model_path = self.config.attention_ensemble_model_path\n",
    "            logger.info(f\"Loading Attention Ensemble model AND weights from: {model_path}\")\n",
    "            if not model_path.exists():\n",
    "                raise FileNotFoundError(f\"Attention ensemble model file not found at: {model_path}\")\n",
    "            try:\n",
    "                model = torch.load(\n",
    "                    model_path,\n",
    "                    map_location=torch.device('cpu'),\n",
    "                    weights_only=False\n",
    "                )\n",
    "                logger.info(f\"Successfully loaded model structure and weights from {model_path}\")\n",
    "                final_layer_name = 'fc'\n",
    "                if hasattr(model, final_layer_name):\n",
    "                    final_layer = getattr(model, final_layer_name)\n",
    "                    if isinstance(final_layer, nn.Linear):\n",
    "                        num_ftrs = final_layer.in_features\n",
    "                        if final_layer.out_features != num_classes:\n",
    "                            logger.warning(f\"Adapting final layer '{final_layer_name}' of loaded attention model. Output: {final_layer.out_features} -> {num_classes}\")\n",
    "                            setattr(model, final_layer_name, nn.Linear(num_ftrs, num_classes))\n",
    "                        else:\n",
    "                            logger.info(f\"Final layer '{final_layer_name}' already matches class count ({num_classes}).\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Attribute '{final_layer_name}' is not nn.Linear. Manual adaptation might be needed.\")\n",
    "                else:\n",
    "                    logger.warning(f\"Could not find final layer '{final_layer_name}'. Assuming loaded model output is correct.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load or adapt attention ensemble model from {model_path}: {e}\")\n",
    "                raise\n",
    "        elif self.config.model_name == \"ensemblenet\":\n",
    "            model = EnsembleNet(num_classes, embed_dim=768)\n",
    "\n",
    "        elif self.config.model_name == \"ensemblenet0\":\n",
    "            model = EnsembleNet(num_classes, embed_dim=768)\n",
    "            weights = [\n",
    "                \"artifacts/training/model_mobilenet_best.pth\",\n",
    "                \"artifacts/training/model_mnasnet_best.pth\",\n",
    "                \"artifacts/training/model_squeezenet_best.pth\"\n",
    "            ]\n",
    "                        \n",
    "            model.load_backbone_weights(weights, strict=False)\n",
    "            model.freeze_backbones()\n",
    "        elif self.config.model_name == \"ensemblenet1\":\n",
    "            model = torch.load(\n",
    "                \"artifacts/training/model_ensemblenet0_best.pth\",\n",
    "                map_location=torch.device('cpu'),\n",
    "                weights_only=False\n",
    "            )\n",
    "            model.unfreeze_backbones()\n",
    "            \n",
    "        elif self.config.model_name == \"ensemblenet2\":\n",
    "            model = EnsembleNet(num_classes, embed_dim=768)\n",
    "            weights = [\n",
    "                \"artifacts/training/model_mobilenet_best.pth\",\n",
    "                \"artifacts/training/model_mnasnet_best.pth\",\n",
    "                \"artifacts/training/model_squeezenet_best.pth\"\n",
    "            ]\n",
    "                        \n",
    "            model.load_backbone_weights(weights, strict=False)\n",
    "            model.unfreeze_backbones()\n",
    "\n",
    "        elif self.config.model_name == \"squeezenet\":\n",
    "            model = models.squeezenet1_1(weights=models.SqueezeNet1_1_Weights)\n",
    "            log_msg = f\"Loading ViT-B/16 with {'ImageNet pretrained' if use_pretrained else 'random'} weights.\"\n",
    "            logger.info(log_msg)\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Dropout(p=0.5, inplace=True),\n",
    "                nn.Conv2d(512, num_classes, kernel_size=(1, 1)),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "            )\n",
    "        elif self.config.model_name == \"shufflenet\":\n",
    "            model = models.shufflenet_v2_x0_5(weights=models.ShuffleNet_V2_X0_5_Weights)\n",
    "            in_features = model._stage_out_channels[-1]\n",
    "            model.fc = nn.Linear(in_features, num_classes)\n",
    "            \n",
    "        elif self.config.model_name == \"mobilenet\":\n",
    "            model = models.mobilenet_v3_small(weights=models.MobileNet_V3_Small_Weights)\n",
    "            lastconv_output_channels = 576\n",
    "            last_channel = 1024\n",
    "            dropout = 0.2\n",
    "            model.classifier = nn.Sequential(\n",
    "                nn.Linear(lastconv_output_channels, last_channel),\n",
    "                nn.Hardswish(inplace=True),\n",
    "                nn.Dropout(p=dropout, inplace=True),\n",
    "                nn.Linear(last_channel, num_classes),\n",
    "            )\n",
    "        elif self.config.model_name == \"attention_ensemble_better\":\n",
    "            pass\n",
    "        elif self.config.model_name == \"mnasnet\":\n",
    "            model = models.mnasnet0_5(weights=models.MNASNet0_5_Weights)\n",
    "            dropout = 0.2\n",
    "            model.classifier = nn.Sequential(nn.Dropout(p=dropout, inplace=True), nn.Linear(1280, num_classes))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model name: {self.config.model_name}\")\n",
    "\n",
    "        self.model = model.to(self.device)\n",
    "        logger.info(f\"Model '{self.config.model_name}' built/loaded and moved to {self.device}.\")\n",
    "        \n",
    "        # Update hyperparameters CSV with model info after building\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def compute_dataset_mean_std(self, data_dir, image_size=(224, 224), batch_size=64, sample_limit=None):\n",
    "        \"\"\"\n",
    "        Computes mean and std for all images in a directory (ImageFolder structure).\n",
    "        Uses cached .npy arrays if available, otherwise computes and saves them.\n",
    "        \"\"\"\n",
    "        mean_path = Path(data_dir) / \"mean.npy\"\n",
    "        std_path = Path(data_dir) / \"std.npy\"\n",
    "\n",
    "        if mean_path.exists() and std_path.exists():\n",
    "            logger.info(f\"Loading cached mean/std from {mean_path} and {std_path}\")\n",
    "            mean = np.load(mean_path)\n",
    "            std = np.load(std_path)\n",
    "            return mean.tolist(), std.tolist()\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        n_images = 0\n",
    "        mean = 0.\n",
    "        std = 0.\n",
    "        for i, (imgs, _) in enumerate(loader):\n",
    "            if sample_limit and n_images >= sample_limit:\n",
    "                break\n",
    "            imgs = imgs.view(imgs.size(0), imgs.size(1), -1)\n",
    "            mean += imgs.mean(2).sum(0)\n",
    "            std += imgs.std(2).sum(0)\n",
    "            n_images += imgs.size(0)\n",
    "            if sample_limit and n_images >= sample_limit:\n",
    "                break\n",
    "\n",
    "        mean /= n_images\n",
    "        std /= n_images\n",
    "\n",
    "        np.save(mean_path, mean.cpu().numpy() if hasattr(mean, \"cpu\") else mean)\n",
    "        np.save(std_path, std.cpu().numpy() if hasattr(std, \"cpu\") else std)\n",
    "        logger.info(f\"Saved computed mean/std to {mean_path} and {std_path}\")\n",
    "\n",
    "        return mean.tolist(), std.tolist()\n",
    "\n",
    "    def prepare_data_loaders(self):\n",
    "        \"\"\"Prepares DataLoaders using pre-split train, validation, and test directories.\"\"\"\n",
    "        img_height, img_width, _ = self.config.params_image_size\n",
    "        target_size = (img_height, img_width)\n",
    "        mean, std = self.compute_dataset_mean_std(self.config.train_data_path)\n",
    "\n",
    "        basic_transform = transforms.Compose([\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "\n",
    "        if self.config.params_augmentation:\n",
    "            train_transform = transforms.Compose([\n",
    "                transforms.Resize(target_size),\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std)\n",
    "            ])\n",
    "            logger.info(\"Using data augmentation for training.\")\n",
    "        else:\n",
    "            train_transform = basic_transform\n",
    "            logger.info(\"Not using data augmentation for training.\")\n",
    "\n",
    "        logger.info(f\"Loading training data from: {self.config.train_data_path}\")\n",
    "        train_dataset = datasets.ImageFolder(\n",
    "            root=self.config.train_data_path,\n",
    "            transform=train_transform\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Loading validation data from: {self.config.valid_data_path}\")\n",
    "        valid_dataset = datasets.ImageFolder(\n",
    "            root=self.config.valid_data_path,\n",
    "            transform=basic_transform\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Loading test data from: {self.config.test_data_path}\")\n",
    "        test_dataset = datasets.ImageFolder(\n",
    "            root=self.config.test_data_path,\n",
    "            transform=basic_transform\n",
    "        )\n",
    "\n",
    "        if not (train_dataset.classes == valid_dataset.classes == test_dataset.classes):\n",
    "            logger.warning(\"Class inconsistency detected between train/validation/test splits!\")\n",
    "        logger.info(f\"Found {len(train_dataset.classes)} classes.\")\n",
    "        if len(train_dataset.classes) != self.config.num_classes:\n",
    "            logger.warning(f\"Number of classes found in data ({len(train_dataset.classes)}) \"\n",
    "                          f\"does not match params.yaml ({self.config.num_classes}). Check data paths and params.\")\n",
    "\n",
    "        num_workers = min(os.cpu_count() // 2, 8) if os.cpu_count() else 4\n",
    "        logger.info(f\"Using {num_workers} workers for DataLoaders.\")\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        self.valid_loader = DataLoader(\n",
    "            dataset=valid_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            dataset=test_dataset,\n",
    "            batch_size=self.config.params_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        logger.info(f\"DataLoaders prepared:\")\n",
    "        logger.info(f\"  Training samples: {len(train_dataset)}, Batches: {len(self.train_loader)}\")\n",
    "        logger.info(f\"  Validation samples: {len(valid_dataset)}, Batches: {len(self.valid_loader)}\")\n",
    "        logger.info(f\"  Test samples: {len(test_dataset)}, Batches: {len(self.test_loader)}\")\n",
    "        \n",
    "        # Update hyperparameters with dataset info\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: nn.Module):\n",
    "        logger.info(f\"Saving model to: {path}\")\n",
    "        torch.save(model, path)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Enhanced training method with comprehensive CSV logging and Cosine Annealing LR\"\"\"\n",
    "        if not self.model or not self.train_loader or not self.valid_loader:\n",
    "            logger.error(\"Model or DataLoaders not initialized. Call build_model() and prepare_data_loaders() first.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting training for model: {self.config.model_name}...\")\n",
    "        logger.info(f\"Metrics will be logged to: {self.metrics_csv_path}\")\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=self.config.params_learning_rate)\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=1-1/math.e,patience=self.config.params_early_stopping_patience // 4)\n",
    "        \n",
    "        logger.info(f\"Using OneCycleLR scheduler with T_max={self.config.params_epochs}, eta_min=1e-6\")\n",
    "\n",
    "        best_valid_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "\n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            # Training Phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.config.params_epochs} [Train]', leave=False)\n",
    "            for inputs, labels in train_pbar:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                if self.config.model_name == \"attention_ensemble\":\n",
    "                    logits = outputs\n",
    "                    loss = F.cross_entropy(logits, labels)\n",
    "                    display_loss = loss.item()\n",
    "                elif isinstance(outputs, models.inception.InceptionOutputs) and self.model.training:\n",
    "                    loss_main = criterion(outputs.logits, labels)\n",
    "                    loss_aux = criterion(outputs.aux_logits, labels)\n",
    "                    loss = loss_main + 0.4 * loss_aux\n",
    "                    display_loss = loss.item()\n",
    "                else:\n",
    "                    outputs_logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                    loss = criterion(outputs_logits, labels)\n",
    "                    display_loss = loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += display_loss * inputs.size(0)\n",
    "                train_pbar.set_postfix({'loss': f'{display_loss:.4f}', 'lr': f\"{optimizer.param_groups[0]['lr']:.1e}\"})\n",
    "            avg_train_loss = train_loss / len(self.train_loader.dataset)\n",
    "\n",
    "            # Validation Phase\n",
    "            self.model.eval()\n",
    "            valid_loss = 0.0\n",
    "            all_labels = []\n",
    "            all_predictions = []\n",
    "            valid_pbar = tqdm(self.valid_loader, desc=f'Epoch {epoch+1}/{self.config.params_epochs} [Valid]', leave=False)\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valid_pbar:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(inputs)\n",
    "                    outputs_logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                    loss = criterion(outputs_logits, labels)\n",
    "                    valid_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = torch.max(outputs_logits.data, 1)\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                    all_predictions.extend(predicted.cpu().numpy())\n",
    "                    valid_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            avg_valid_loss = valid_loss / len(self.valid_loader.dataset)\n",
    "            scheduler.step(avg_valid_loss)\n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(all_labels, all_predictions) * 100\n",
    "            precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "            recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "            f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            logger.info(f'Epoch {epoch+1} Summary -> Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}, '\n",
    "                       f'Valid Acc: {accuracy:.2f}%, Prec: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, LR: {current_lr:.2e}')\n",
    "\n",
    "            # Determine if this is the best model\n",
    "            is_best = avg_valid_loss < best_valid_loss\n",
    "            \n",
    "            # Log metrics to CSV\n",
    "            self.log_metrics_to_csv(\n",
    "                epoch=epoch,\n",
    "                train_loss=avg_train_loss,\n",
    "                valid_loss=avg_valid_loss,\n",
    "                accuracy=accuracy,\n",
    "                precision=precision,\n",
    "                recall=recall,\n",
    "                f1=f1,\n",
    "                learning_rate=current_lr,\n",
    "                is_best_model=is_best,\n",
    "                early_stopping_counter=early_stopping_counter\n",
    "            )\n",
    "            \n",
    "            # Save models\n",
    "            self.save_model(path=self.config.last_model_path, model=self.model)\n",
    "            \n",
    "            if is_best:\n",
    "                best_valid_loss = avg_valid_loss\n",
    "                early_stopping_counter = 0\n",
    "                self.save_model(path=self.config.best_model_path, model=self.model)\n",
    "                logger.info(f'>>> Best model saved (Epoch {epoch+1}) with Valid Loss: {avg_valid_loss:.4f}')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                logger.info(f'Validation loss did not improve. Counter: {early_stopping_counter}/{self.config.params_early_stopping_patience}')\n",
    "                if early_stopping_counter >= self.config.params_early_stopping_patience:\n",
    "                    logger.warning('--- Early stopping triggered ---')\n",
    "                    break\n",
    "\n",
    "        logger.info(f'Training finished for {self.config.model_name}.')\n",
    "        logger.info(f'Best validation loss achieved: {best_valid_loss:.4f}')\n",
    "        logger.info(f'Best model saved at: {self.config.best_model_path}')\n",
    "        logger.info(f'Last model saved at: {self.config.last_model_path}')\n",
    "        logger.info(f'Training metrics saved to: {self.metrics_csv_path}')\n",
    "        logger.info(f'Hyperparameters saved to: {self.hyperparams_csv_path}')\n",
    "        \n",
    "        # Create a final summary CSV\n",
    "        self.create_training_summary()\n",
    "        \n",
    "    def create_training_summary(self):\n",
    "        \"\"\"Create a summary CSV with final training results\"\"\"\n",
    "        try:\n",
    "            # Read the metrics CSV to get final results\n",
    "            df_metrics = pd.read_csv(self.metrics_csv_path)\n",
    "            if not df_metrics.empty:\n",
    "                best_epoch_row = df_metrics.loc[df_metrics['is_best_model'] == True]\n",
    "                final_epoch_row = df_metrics.iloc[-1]\n",
    "                \n",
    "                summary_data = {\n",
    "                    'model_name': [self.config.model_name],\n",
    "                    'total_epochs': [len(df_metrics)],\n",
    "                    'best_epoch': [best_epoch_row['epoch'].iloc[0] if not best_epoch_row.empty else 'N/A'],\n",
    "                    'best_valid_loss': [best_epoch_row['valid_loss'].iloc[0] if not best_epoch_row.empty else 'N/A'],\n",
    "                    'best_accuracy': [best_epoch_row['accuracy'].iloc[0] if not best_epoch_row.empty else 'N/A'],\n",
    "                    'final_train_loss': [final_epoch_row['train_loss']],\n",
    "                    'final_valid_loss': [final_epoch_row['valid_loss']],\n",
    "                    'final_accuracy': [final_epoch_row['accuracy']],\n",
    "                    'final_f1_score': [final_epoch_row['f1_score']],\n",
    "                    'early_stopped': [final_epoch_row['early_stopping_counter'] >= self.config.params_early_stopping_patience],\n",
    "                    'training_completed': [datetime.now().isoformat()]\n",
    "                }\n",
    "                \n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                summary_path = Path(self.config.root_dir) / \"logs\" / f\"training_summary_{self.config.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                summary_df.to_csv(summary_path, index=False)\n",
    "                \n",
    "                logger.info(f\"Training summary saved to: {summary_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating training summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:23:24,739: INFO: utils: yaml file config/config.yaml loaded successfully:]\n",
      "[2025-06-21 00:23:24,746: INFO: utils: yaml file config/params.yaml loaded successfully:]\n",
      "[2025-06-21 00:23:24,747: INFO: utils: created directory at: artifacts:]\n",
      "[2025-06-21 00:23:24,748: INFO: utils: created directory at: artifacts/training:]\n",
      "[2025-06-21 00:23:24,748: INFO: utils: created directory at: artifacts/training/logs:]\n",
      "[2025-06-21 00:23:24,748: INFO: 2355115549: Number of classes set to: 257:]\n",
      "[2025-06-21 00:23:24,749: INFO: 2355115549: Use pretrained weights: True (Based on PRETRAINED_WEIGHTS: IMAGENET1K_V1):]\n",
      "[2025-06-21 00:23:24,751: INFO: 2355115549: Validated 257 classes in artifacts/data_ingestion/train, matching params.yaml.:]\n",
      "[2025-06-21 00:23:24,752: INFO: utils: created directory at: artifacts/training:]\n",
      "[2025-06-21 00:23:24,752: INFO: utils: created directory at: artifacts/training:]\n",
      "[2025-06-21 00:23:24,753: INFO: 2355115549: Run Learning Rate Finder: True:]\n",
      "[2025-06-21 00:23:24,753: INFO: utils: created directory at: artifacts/training:]\n",
      "[2025-06-21 00:23:24,754: INFO: 245092438: Loaded configuration for model: ensemblenet1:]\n",
      "[2025-06-21 00:23:24,754: INFO: 245092438: Target Image Size: [224, 224, 3]:]\n",
      "[2025-06-21 00:23:24,754: INFO: 245092438: Number of classes: 257:]\n",
      "[2025-06-21 00:23:24,755: INFO: 245092438: Initial Learning Rate from params: 0.01:]\n",
      "[2025-06-21 00:23:24,755: INFO: 245092438: Run LR Finder: True:]\n",
      "[2025-06-21 00:23:24,756: INFO: 470616600: Using device: cuda:]\n",
      "[2025-06-21 00:23:24,758: INFO: 470616600: Metrics CSV initialized: artifacts/training/logs/training_metrics_ensemblenet1.csv:]\n",
      "[2025-06-21 00:23:24,762: INFO: 470616600: Hyperparameters saved to: artifacts/training/logs/hyperparameters_ensemblenet1.csv:]\n",
      "[2025-06-21 00:23:24,763: INFO: 470616600: Loading cached mean/std from artifacts/data_ingestion/train/mean.npy and artifacts/data_ingestion/train/std.npy:]\n",
      "[2025-06-21 00:23:24,765: INFO: 470616600: Using data augmentation for training.:]\n",
      "[2025-06-21 00:23:24,765: INFO: 470616600: Loading training data from: artifacts/data_ingestion/train:]\n",
      "[2025-06-21 00:23:24,830: INFO: 470616600: Loading validation data from: artifacts/data_ingestion/val:]\n",
      "[2025-06-21 00:23:24,845: INFO: 470616600: Loading test data from: artifacts/data_ingestion/test:]\n",
      "[2025-06-21 00:23:24,864: INFO: 470616600: Found 257 classes.:]\n",
      "[2025-06-21 00:23:24,864: INFO: 470616600: Using 6 workers for DataLoaders.:]\n",
      "[2025-06-21 00:23:24,865: INFO: 470616600: DataLoaders prepared::]\n",
      "[2025-06-21 00:23:24,865: INFO: 470616600:   Training samples: 21307, Batches: 1332:]\n",
      "[2025-06-21 00:23:24,866: INFO: 470616600:   Validation samples: 3273, Batches: 205:]\n",
      "[2025-06-21 00:23:24,867: INFO: 470616600:   Test samples: 6027, Batches: 377:]\n",
      "[2025-06-21 00:23:24,869: INFO: 470616600: Hyperparameters saved to: artifacts/training/logs/hyperparameters_ensemblenet1.csv:]\n",
      "[2025-06-21 00:23:24,869: INFO: 470616600: Building model: ensemblenet1:]\n",
      "[2025-06-21 00:23:24,938: INFO: ensemblenet: Unfrozen backbone 0 (mobilenet_v3_small):]\n",
      "[2025-06-21 00:23:24,939: INFO: ensemblenet: Unfrozen backbone 1 (mnasnet0_5):]\n",
      "[2025-06-21 00:23:24,939: INFO: ensemblenet: Unfrozen backbone 2 (squeezenet1_1):]\n",
      "[2025-06-21 00:23:24,965: INFO: 470616600: Model 'ensemblenet1' built/loaded and moved to cuda.:]\n",
      "[2025-06-21 00:23:24,967: INFO: 470616600: Hyperparameters saved to: artifacts/training/logs/hyperparameters_ensemblenet1.csv:]\n",
      "[2025-06-21 00:23:24,967: INFO: 245092438: Setting LR Finder iterations to: 1332:]\n",
      "[2025-06-21 00:23:24,968: INFO: 470616600: No previously saved suggested learning rate found:]\n",
      "[2025-06-21 00:23:24,968: INFO: 470616600: --- Running Learning Rate Finder (Ignite) ---:]\n",
      "[2025-06-21 00:23:24,969: INFO: 470616600: Using end_lr=0.001, num_iter=3000, step_mode='exp':]\n",
      "[2025-06-21 00:23:25,060: INFO: engine: Engine run starting with max_epochs=1.:]\n",
      "[2025-06-21 00:26:14,211: INFO: lr_finder: Stopping early, the loss has diverged:]\n",
      "[2025-06-21 00:26:14,214: INFO: engine: Terminate signaled. Engine will stop after current iteration is finished.:]\n",
      "[2025-06-21 00:26:14,216: INFO: engine: Engine run finished. Time taken: 00:02:49.154:]\n",
      "[2025-06-21 00:26:14,537: INFO: 470616600: Plotting LR Finder results...:]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:26:17,690: INFO: 470616600: LR Finder plot saved to: artifacts/training/lr_finder_plot_ensemblenet1.png:]\n",
      "[2025-06-21 00:26:17,690: INFO: 470616600: Ignite LR Finder suggested learning rate: 5.54e-06:]\n",
      "[2025-06-21 00:26:17,698: INFO: 470616600: LR Finder results saved to: artifacts/training/logs/lr_finder_results_ensemblenet1.csv:]\n",
      "[2025-06-21 00:26:17,704: INFO: 470616600: Suggested learning rate (5.54e-06) saved to: artifacts/training/logs/lr_finder_results_ensemblenet1_suggested_lr.txt:]\n",
      "[2025-06-21 00:26:17,706: INFO: 470616600: --- Learning Rate Finder (Ignite) Finished ---:]\n",
      "[2025-06-21 00:26:17,707: INFO: 245092438: Overriding LR from params. Using Ignite LR Finder result: 5.54e-06:]\n",
      "[2025-06-21 00:26:17,707: INFO: 470616600: Starting training for model: ensemblenet1...:]\n",
      "[2025-06-21 00:26:17,708: INFO: 470616600: Metrics will be logged to: artifacts/training/logs/training_metrics_ensemblenet1.csv:]\n",
      "[2025-06-21 00:26:17,711: INFO: 470616600: Using OneCycleLR scheduler with T_max=100, eta_min=1e-6:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:29:38,765: INFO: 470616600: Epoch 1 Summary -> Train Loss: 0.2802, Valid Loss: 1.0010, Valid Acc: 76.75%, Prec: 0.7901, Recall: 0.7675, F1: 0.7675, LR: 5.54e-06:]\n",
      "[2025-06-21 00:29:38,766: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:29:38,935: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 00:29:39,072: INFO: 470616600: >>> Best model saved (Epoch 1) with Valid Loss: 1.0010:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:32:59,936: INFO: 470616600: Epoch 2 Summary -> Train Loss: 0.2457, Valid Loss: 0.9940, Valid Acc: 77.15%, Prec: 0.7913, Recall: 0.7715, F1: 0.7714, LR: 5.54e-06:]\n",
      "[2025-06-21 00:32:59,938: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 00:33:00,117: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 00:33:00,279: INFO: 470616600: >>> Best model saved (Epoch 2) with Valid Loss: 0.9940:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:36:21,147: INFO: 470616600: Epoch 3 Summary -> Train Loss: 0.2247, Valid Loss: 0.9919, Valid Acc: 77.54%, Prec: 0.7921, Recall: 0.7754, F1: 0.7748, LR: 5.54e-06:]\n",
      "[2025-06-21 00:36:21,151: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:36:21,335: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 00:36:21,493: INFO: 470616600: >>> Best model saved (Epoch 3) with Valid Loss: 0.9919:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:39:41,011: INFO: 470616600: Epoch 4 Summary -> Train Loss: 0.2124, Valid Loss: 0.9778, Valid Acc: 77.67%, Prec: 0.7922, Recall: 0.7767, F1: 0.7760, LR: 5.54e-06:]\n",
      "[2025-06-21 00:39:41,012: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 00:39:41,149: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 00:39:41,286: INFO: 470616600: >>> Best model saved (Epoch 4) with Valid Loss: 0.9778:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:42:57,806: INFO: 470616600: Epoch 5 Summary -> Train Loss: 0.2015, Valid Loss: 0.9813, Valid Acc: 77.73%, Prec: 0.7935, Recall: 0.7773, F1: 0.7767, LR: 5.54e-06:]\n",
      "[2025-06-21 00:42:57,807: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 00:42:57,946: INFO: 470616600: Validation loss did not improve. Counter: 1/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:46:18,028: INFO: 470616600: Epoch 6 Summary -> Train Loss: 0.1872, Valid Loss: 0.9820, Valid Acc: 77.48%, Prec: 0.7901, Recall: 0.7748, F1: 0.7745, LR: 5.54e-06:]\n",
      "[2025-06-21 00:46:18,029: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:46:18,216: INFO: 470616600: Validation loss did not improve. Counter: 2/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:49:38,924: INFO: 470616600: Epoch 7 Summary -> Train Loss: 0.1761, Valid Loss: 0.9793, Valid Acc: 78.00%, Prec: 0.7962, Recall: 0.7800, F1: 0.7801, LR: 3.50e-06:]\n",
      "[2025-06-21 00:49:38,925: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:49:39,084: INFO: 470616600: Validation loss did not improve. Counter: 3/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:52:59,590: INFO: 470616600: Epoch 8 Summary -> Train Loss: 0.1736, Valid Loss: 0.9724, Valid Acc: 77.60%, Prec: 0.7907, Recall: 0.7760, F1: 0.7758, LR: 3.50e-06:]\n",
      "[2025-06-21 00:52:59,592: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 00:52:59,782: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 00:52:59,920: INFO: 470616600: >>> Best model saved (Epoch 8) with Valid Loss: 0.9724:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:56:19,425: INFO: 470616600: Epoch 9 Summary -> Train Loss: 0.1687, Valid Loss: 0.9626, Valid Acc: 77.97%, Prec: 0.7954, Recall: 0.7797, F1: 0.7795, LR: 3.50e-06:]\n",
      "[2025-06-21 00:56:19,426: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:56:19,608: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 00:56:19,745: INFO: 470616600: >>> Best model saved (Epoch 9) with Valid Loss: 0.9626:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:59:40,088: INFO: 470616600: Epoch 10 Summary -> Train Loss: 0.1675, Valid Loss: 0.9736, Valid Acc: 78.00%, Prec: 0.7958, Recall: 0.7800, F1: 0.7799, LR: 3.50e-06:]\n",
      "[2025-06-21 00:59:40,089: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 00:59:40,248: INFO: 470616600: Validation loss did not improve. Counter: 1/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:02:59,734: INFO: 470616600: Epoch 11 Summary -> Train Loss: 0.1663, Valid Loss: 0.9707, Valid Acc: 78.06%, Prec: 0.7947, Recall: 0.7806, F1: 0.7801, LR: 3.50e-06:]\n",
      "[2025-06-21 01:02:59,735: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:02:59,898: INFO: 470616600: Validation loss did not improve. Counter: 2/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:06:19,597: INFO: 470616600: Epoch 12 Summary -> Train Loss: 0.1655, Valid Loss: 0.9686, Valid Acc: 77.97%, Prec: 0.7946, Recall: 0.7797, F1: 0.7792, LR: 2.21e-06:]\n",
      "[2025-06-21 01:06:19,598: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:06:19,768: INFO: 470616600: Validation loss did not improve. Counter: 3/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:09:39,191: INFO: 470616600: Epoch 13 Summary -> Train Loss: 0.1550, Valid Loss: 0.9657, Valid Acc: 78.25%, Prec: 0.7972, Recall: 0.7825, F1: 0.7820, LR: 2.21e-06:]\n",
      "[2025-06-21 01:09:39,193: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:09:39,362: INFO: 470616600: Validation loss did not improve. Counter: 4/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:12:59,321: INFO: 470616600: Epoch 14 Summary -> Train Loss: 0.1533, Valid Loss: 0.9740, Valid Acc: 78.43%, Prec: 0.7998, Recall: 0.7843, F1: 0.7841, LR: 2.21e-06:]\n",
      "[2025-06-21 01:12:59,322: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 01:12:59,498: INFO: 470616600: Validation loss did not improve. Counter: 5/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:16:19,020: INFO: 470616600: Epoch 15 Summary -> Train Loss: 0.1514, Valid Loss: 0.9639, Valid Acc: 78.22%, Prec: 0.7970, Recall: 0.7822, F1: 0.7818, LR: 1.40e-06:]\n",
      "[2025-06-21 01:16:19,021: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 01:16:19,178: INFO: 470616600: Validation loss did not improve. Counter: 6/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:19:38,834: INFO: 470616600: Epoch 16 Summary -> Train Loss: 0.1471, Valid Loss: 0.9617, Valid Acc: 78.15%, Prec: 0.7957, Recall: 0.7815, F1: 0.7811, LR: 1.40e-06:]\n",
      "[2025-06-21 01:19:38,837: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:19:38,999: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 01:19:39,165: INFO: 470616600: >>> Best model saved (Epoch 16) with Valid Loss: 0.9617:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:22:58,807: INFO: 470616600: Epoch 17 Summary -> Train Loss: 0.1500, Valid Loss: 0.9657, Valid Acc: 78.12%, Prec: 0.7966, Recall: 0.7812, F1: 0.7813, LR: 1.40e-06:]\n",
      "[2025-06-21 01:22:58,809: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 01:22:58,961: INFO: 470616600: Validation loss did not improve. Counter: 1/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:26:18,489: INFO: 470616600: Epoch 18 Summary -> Train Loss: 0.1448, Valid Loss: 0.9600, Valid Acc: 78.19%, Prec: 0.7959, Recall: 0.7819, F1: 0.7813, LR: 1.40e-06:]\n",
      "[2025-06-21 01:26:18,490: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 01:26:18,704: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 01:26:18,899: INFO: 470616600: >>> Best model saved (Epoch 18) with Valid Loss: 0.9600:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:29:38,419: INFO: 470616600: Epoch 19 Summary -> Train Loss: 0.1458, Valid Loss: 0.9676, Valid Acc: 78.00%, Prec: 0.7944, Recall: 0.7800, F1: 0.7795, LR: 1.40e-06:]\n",
      "[2025-06-21 01:29:38,421: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 01:29:38,617: INFO: 470616600: Validation loss did not improve. Counter: 1/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:32:58,049: INFO: 470616600: Epoch 20 Summary -> Train Loss: 0.1434, Valid Loss: 0.9684, Valid Acc: 77.91%, Prec: 0.7932, Recall: 0.7791, F1: 0.7787, LR: 1.40e-06:]\n",
      "[2025-06-21 01:32:58,050: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:32:58,209: INFO: 470616600: Validation loss did not improve. Counter: 2/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:36:18,085: INFO: 470616600: Epoch 21 Summary -> Train Loss: 0.1423, Valid Loss: 0.9623, Valid Acc: 78.19%, Prec: 0.7963, Recall: 0.7819, F1: 0.7815, LR: 8.84e-07:]\n",
      "[2025-06-21 01:36:18,087: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:36:18,233: INFO: 470616600: Validation loss did not improve. Counter: 3/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:39:37,827: INFO: 470616600: Epoch 22 Summary -> Train Loss: 0.1435, Valid Loss: 0.9639, Valid Acc: 78.00%, Prec: 0.7943, Recall: 0.7800, F1: 0.7798, LR: 8.84e-07:]\n",
      "[2025-06-21 01:39:37,829: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:39:38,042: INFO: 470616600: Validation loss did not improve. Counter: 4/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:42:57,533: INFO: 470616600: Epoch 23 Summary -> Train Loss: 0.1458, Valid Loss: 0.9619, Valid Acc: 78.19%, Prec: 0.7964, Recall: 0.7819, F1: 0.7817, LR: 8.84e-07:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:42:57,534: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 01:42:57,713: INFO: 470616600: Validation loss did not improve. Counter: 5/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:46:17,462: INFO: 470616600: Epoch 24 Summary -> Train Loss: 0.1349, Valid Loss: 0.9638, Valid Acc: 78.03%, Prec: 0.7944, Recall: 0.7803, F1: 0.7800, LR: 5.59e-07:]\n",
      "[2025-06-21 01:46:17,463: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:46:17,621: INFO: 470616600: Validation loss did not improve. Counter: 6/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:49:37,251: INFO: 470616600: Epoch 25 Summary -> Train Loss: 0.1362, Valid Loss: 0.9637, Valid Acc: 78.06%, Prec: 0.7945, Recall: 0.7806, F1: 0.7803, LR: 5.59e-07:]\n",
      "[2025-06-21 01:49:37,252: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:49:37,418: INFO: 470616600: Validation loss did not improve. Counter: 7/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:52:57,118: INFO: 470616600: Epoch 26 Summary -> Train Loss: 0.1334, Valid Loss: 0.9650, Valid Acc: 78.15%, Prec: 0.7966, Recall: 0.7815, F1: 0.7817, LR: 5.59e-07:]\n",
      "[2025-06-21 01:52:57,119: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:52:57,281: INFO: 470616600: Validation loss did not improve. Counter: 8/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:56:16,863: INFO: 470616600: Epoch 27 Summary -> Train Loss: 0.1350, Valid Loss: 0.9622, Valid Acc: 78.15%, Prec: 0.7963, Recall: 0.7815, F1: 0.7813, LR: 3.53e-07:]\n",
      "[2025-06-21 01:56:16,865: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 01:56:17,062: INFO: 470616600: Validation loss did not improve. Counter: 9/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 01:59:37,036: INFO: 470616600: Epoch 28 Summary -> Train Loss: 0.1343, Valid Loss: 0.9646, Valid Acc: 78.15%, Prec: 0.7965, Recall: 0.7815, F1: 0.7813, LR: 3.53e-07:]\n",
      "[2025-06-21 01:59:37,039: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 01:59:37,210: INFO: 470616600: Validation loss did not improve. Counter: 10/11:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 02:02:57,110: INFO: 470616600: Epoch 29 Summary -> Train Loss: 0.1336, Valid Loss: 0.9648, Valid Acc: 78.25%, Prec: 0.7975, Recall: 0.7825, F1: 0.7821, LR: 3.53e-07:]\n",
      "[2025-06-21 02:02:57,111: INFO: 470616600: Saving model to: artifacts/training/model_ensemblenet1_last.pth:]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 02:02:57,289: INFO: 470616600: Validation loss did not improve. Counter: 11/11:]\n",
      "[2025-06-21 02:02:57,290: WARNING: 470616600: --- Early stopping triggered ---:]\n",
      "[2025-06-21 02:02:57,290: INFO: 470616600: Training finished for ensemblenet1.:]\n",
      "[2025-06-21 02:02:57,290: INFO: 470616600: Best validation loss achieved: 0.9600:]\n",
      "[2025-06-21 02:02:57,291: INFO: 470616600: Best model saved at: artifacts/training/model_ensemblenet1_best.pth:]\n",
      "[2025-06-21 02:02:57,291: INFO: 470616600: Last model saved at: artifacts/training/model_ensemblenet1_last.pth:]\n",
      "[2025-06-21 02:02:57,292: INFO: 470616600: Training metrics saved to: artifacts/training/logs/training_metrics_ensemblenet1.csv:]\n",
      "[2025-06-21 02:02:57,292: INFO: 470616600: Hyperparameters saved to: artifacts/training/logs/hyperparameters_ensemblenet1.csv:]\n",
      "[2025-06-21 02:02:57,455: INFO: 470616600: Training summary saved to: artifacts/training/logs/training_summary_ensemblenet1_20250621_020257.csv:]\n",
      "[2025-06-21 02:02:57,460: INFO: 245092438: <<<<< Training Pipeline Completed Successfully >>>>>:]\n"
     ]
    }
   ],
   "source": [
    "# In the main execution cell (at the end)\n",
    "# %%\n",
    "import traceback\n",
    "import dataclasses # Import dataclasses to modify config\n",
    "import torchvision\n",
    "torch.serialization.add_safe_globals([\n",
    "    torchvision.models.mobilenetv3.MobileNetV3,\n",
    "    torchvision.models.mnasnet.MNASNet,\n",
    "    torchvision.models.squeezenet.SqueezeNet,\n",
    "])\n",
    "try:\n",
    "    # 1. Get Configuration\n",
    "    config_manager = ConfigurationManager()\n",
    "    training_config = config_manager.get_training_config()\n",
    "    logger.info(f\"Loaded configuration for model: {training_config.model_name}\")\n",
    "    logger.info(f\"Target Image Size: {training_config.params_image_size}\")\n",
    "    logger.info(f\"Number of classes: {training_config.num_classes}\")\n",
    "    logger.info(f\"Initial Learning Rate from params: {training_config.params_learning_rate}\")\n",
    "    logger.info(f\"Run LR Finder: {training_config.params_find_lr}\")\n",
    "\n",
    "    # 2. Initialize Training Class\n",
    "    training = Training(training_config) # Pass the config object\n",
    "\n",
    "    # 3. Prepare Data Loaders\n",
    "    training.prepare_data_loaders()\n",
    "\n",
    "    # 4. Build the selected model\n",
    "    training.build_model()\n",
    "\n",
    "    # --- 5. Run LR Finder (if enabled) ---\n",
    "    if training.config.params_find_lr:\n",
    "        # --- Call the Ignite version ---\n",
    "        # Adjust parameters like end_lr, num_iter if needed\n",
    "        num_iterations_finder = len(training.train_loader) # Run for approx one epoch\n",
    "        logger.info(f\"Setting LR Finder iterations to: {num_iterations_finder}\")\n",
    "        suggested_lr = training.find_optimal_lr_ignite(1e-6, 1e-3, num_iter=3000)\n",
    "        # --------------------------------\n",
    "        # suggested_lr = 1e-3\n",
    "        if suggested_lr is not None:\n",
    "            # Update the learning rate in the config object\n",
    "            training.config = dataclasses.replace(training.config, params_learning_rate=suggested_lr)\n",
    "            logger.info(f\"Overriding LR from params. Using Ignite LR Finder result: {training.config.params_learning_rate:.2e}\")\n",
    "        else:\n",
    "            logger.warning(f\"Ignite LR Finder did not provide a suggestion. Using original LR: {training.config.params_learning_rate:.2e}\")\n",
    "            # No change needed, keep the original LR\n",
    "    else:\n",
    "        logger.info(\"Skipping LR Finder as per configuration.\")\n",
    "    # --- End LR Finder ---\n",
    "\n",
    "    # 6. Start Training (uses the potentially updated LR in training.config)\n",
    "    training.train()\n",
    "\n",
    "    logger.info(\"<<<<< Training Pipeline Completed Successfully >>>>>\")\n",
    "\n",
    "# (Keep existing except block, maybe add Ignite specific import errors if needed)\n",
    "except ImportError as e:\n",
    "    if 'ignite' in str(e):\n",
    "         logger.error(\"ImportError: `pytorch-ignite` library not found or import failed. Please install it: `pip install pytorch-ignite matplotlib`\")\n",
    "    elif 'matplotlib' in str(e):\n",
    "         logger.error(\"ImportError: `matplotlib` library not found. Please install it for plotting: `pip install matplotlib`\")\n",
    "    else:\n",
    "         logger.exception(\"An import error occurred:\")\n",
    "# ... rest of your except blocks ...\n",
    "except Exception as e:\n",
    "    logger.exception(\"An unexpected error occurred during the training pipeline:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
