{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gorkemkola/Desktop/Projects/EnsembleNet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/gorkemkola/Desktop/Projects/EnsembleNet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from ensemblenet.entity import DataIngestionConfig\n",
    "from ensemblenet.utils import read_yaml, create_directories\n",
    "from ensemblenet.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH,\n",
    "    ) -> None:\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        # Create all necessary directories defined in config.data_ingestion\n",
    "        create_directories([\n",
    "            Path(self.config.artifacts_root),\n",
    "            Path(self.config.data_ingestion.root_dir),\n",
    "            Path(self.config.data_ingestion.unzip_dir), # Still needed initially\n",
    "            Path(self.config.data_ingestion.train_dir),\n",
    "            Path(self.config.data_ingestion.test_dir),\n",
    "            Path(self.config.data_ingestion.val_dir),\n",
    "        ])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            source_kaggle_dataset_id=config.source_kaggle_dataset_id,\n",
    "            local_data_file=Path(config.local_data_file),\n",
    "            unzip_dir=Path(config.unzip_dir),\n",
    "            train_dir=Path(config.train_dir),\n",
    "            test_dir=Path(config.test_dir),\n",
    "            val_dir=Path(config.val_dir),\n",
    "            cleanup_unzip_dir_after_split=config.cleanup_unzip_dir_after_split # Read from yaml\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 13:30:34,224: INFO: utils: yaml file config/config.yaml loaded successfully:]\n",
      "[2025-06-21 13:30:34,229: INFO: utils: yaml file config/params.yaml loaded successfully:]\n",
      "[2025-06-21 13:30:34,230: INFO: utils: created directory at: artifacts:]\n",
      "[2025-06-21 13:30:34,231: INFO: utils: created directory at: artifacts/data_ingestion:]\n",
      "[2025-06-21 13:30:34,231: INFO: utils: created directory at: artifacts/data_ingestion/extracted_data:]\n",
      "[2025-06-21 13:30:34,232: INFO: utils: created directory at: artifacts/data_ingestion/train:]\n",
      "[2025-06-21 13:30:34,233: INFO: utils: created directory at: artifacts/data_ingestion/test:]\n",
      "[2025-06-21 13:30:34,234: INFO: utils: created directory at: artifacts/data_ingestion/val:]\n",
      "DataIngestionConfig(root_dir=PosixPath('artifacts/data_ingestion'), source_kaggle_dataset_id='jessicali9530/caltech256', local_data_file=PosixPath('artifacts/data_ingestion/caltech256.zip'), unzip_dir=PosixPath('artifacts/data_ingestion/extracted_data'), train_dir=PosixPath('artifacts/data_ingestion/train'), test_dir=PosixPath('artifacts/data_ingestion/test'), val_dir=PosixPath('artifacts/data_ingestion/val'), cleanup_unzip_dir_after_split=True)\n"
     ]
    }
   ],
   "source": [
    "print(ConfigurationManager().get_data_ingestion_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from ensemblenet.utils import logger\n",
    "import os\n",
    "import subprocess\n",
    "import random \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config: DataIngestionConfig\n",
    "        ) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    # ... (download_file method remains the same) ...\n",
    "    def download_file(self):\n",
    "        '''\n",
    "        Description:\n",
    "            Downloads the dataset from Kaggle using the Kaggle API.\n",
    "            Returns a status string.\n",
    "        '''\n",
    "        # (Code from previous answer - no changes needed here)\n",
    "        # Make sure it returns appropriate status strings like:\n",
    "        # \"Skipped - Already Processed\", \"Download Complete\", or raises error\n",
    "        # ... (rest of download_file implementation) ...\n",
    "        try:\n",
    "            dataset_id = self.config.source_kaggle_dataset_id\n",
    "            download_dir = self.config.root_dir # Directory to download the zip file into\n",
    "            local_data_file = self.config.local_data_file\n",
    "\n",
    "            # Check if the zip file exists first\n",
    "            if local_data_file.exists():\n",
    "                logger.info(f\"Zip file '{local_data_file}' already exists.\")\n",
    "                # More robust check: if split dirs are populated, assume everything is done\n",
    "                if self.config.train_dir.exists() and any(f for f in self.config.train_dir.glob('*/*') if f.is_file()): # Check for files within class dirs\n",
    "                     logger.info(\"Split directories seem populated. Skipping download, extraction, and split.\")\n",
    "                     return \"Skipped - Already Processed\"\n",
    "                else:\n",
    "                    logger.info(\"Zip file exists, but split directories seem empty or incomplete. Proceeding.\")\n",
    "                    # No return, continue to check extraction/split\n",
    "            else:\n",
    "                logger.info(f\"Downloading dataset '{dataset_id}' from Kaggle into '{download_dir}'...\")\n",
    "                command = [\n",
    "                    \"kaggle\", \"datasets\", \"download\",\n",
    "                    \"-d\", dataset_id,\n",
    "                    \"-p\", str(download_dir),\n",
    "                ]\n",
    "                result = subprocess.run(command, capture_output=True, text=True, check=False)\n",
    "\n",
    "                if result.returncode == 0:\n",
    "                    expected_zip_filename = dataset_id.split('/')[-1] + \".zip\"\n",
    "                    potential_downloaded_path = download_dir / expected_zip_filename\n",
    "                    actual_local_data_file = self.config.local_data_file\n",
    "\n",
    "                    if not actual_local_data_file.exists():\n",
    "                        if potential_downloaded_path.exists():\n",
    "                            logger.warning(f\"Downloaded file seems to be '{potential_downloaded_path}', but config expects '{actual_local_data_file}'. Renaming.\")\n",
    "                            try:\n",
    "                                potential_downloaded_path.rename(actual_local_data_file)\n",
    "                            except OSError as e:\n",
    "                                logger.error(f\"Error renaming downloaded file: {e}. Trying to copy and remove.\")\n",
    "                                try:\n",
    "                                     shutil.copy2(potential_downloaded_path, actual_local_data_file)\n",
    "                                     potential_downloaded_path.unlink()\n",
    "                                     logger.info(\"Successfully copied and removed original download.\")\n",
    "                                except Exception as copy_e:\n",
    "                                     logger.error(f\"Failed to copy/delete after rename error: {copy_e}\")\n",
    "                                     raise FileNotFoundError(f\"Downloaded file '{potential_downloaded_path}' exists but couldn't be moved/copied to '{actual_local_data_file}'.\")\n",
    "\n",
    "                        else:\n",
    "                            logger.error(f\"Kaggle command succeeded, but expected file '{actual_local_data_file}' (or '{potential_downloaded_path}') not found.\")\n",
    "                            logger.error(f\"Kaggle CLI stdout:\\n{result.stdout}\")\n",
    "                            logger.error(f\"Kaggle CLI stderr:\\n{result.stderr}\")\n",
    "                            raise FileNotFoundError(f\"Expected file '{actual_local_data_file}' not found after download attempt.\")\n",
    "                    logger.info(f\"Successfully downloaded '{dataset_id}' to '{actual_local_data_file}'\")\n",
    "                else:\n",
    "                    # ... (rest of the error handling) ...\n",
    "                    logger.error(f\"Failed to download dataset '{dataset_id}'.\")\n",
    "                    logger.error(f\"Return Code: {result.returncode}\")\n",
    "                    logger.error(f\"Kaggle CLI stdout:\\n{result.stdout}\")\n",
    "                    logger.error(f\"Kaggle CLI stderr:\\n{result.stderr}\")\n",
    "                    if \"401\" in result.stderr or \"authenticate\" in result.stderr.lower():\n",
    "                         logger.error(\"Authentication error: Ensure 'kaggle.json' is correctly placed and configured.\")\n",
    "                    elif \"404\" in result.stderr or \"not found\" in result.stderr.lower():\n",
    "                         logger.error(f\"Dataset '{dataset_id}' not found on Kaggle. Check the dataset ID.\")\n",
    "                    elif \"429\" in result.stderr or \"Too Many Requests\" in result.stderr:\n",
    "                         logger.error(\"Rate limit exceeded. Please wait before trying again.\")\n",
    "                    raise Exception(f\"Kaggle download failed with stderr: {result.stderr}\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "             if \"kaggle\" in str(e):\n",
    "                  logger.error(\"The 'kaggle' command was not found. Please ensure the Kaggle CLI is installed and in your system's PATH.\")\n",
    "                  raise RuntimeError(\"Kaggle CLI not found. Please install it (`pip install kaggle`) and configure it.\") from e\n",
    "             else:\n",
    "                  logger.error(f\"An unexpected FileNotFoundError occurred: {e}\")\n",
    "                  raise e # Re-raise other FileNotFoundError\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during file download: {e}\")\n",
    "            raise e\n",
    "        return \"Download Complete\"\n",
    "\n",
    "    def extract_zip_file(self):\n",
    "        '''\n",
    "        Description:\n",
    "            Extracts the downloaded zip file into the unzip directory.\n",
    "            Returns a status string.\n",
    "        '''\n",
    "        unzip_path = self.config.unzip_dir\n",
    "        local_data_file = self.config.local_data_file\n",
    "\n",
    "        # Check if extraction seems complete AND split seems complete\n",
    "        if self.config.train_dir.exists() and any(f for f in self.config.train_dir.glob('*/*') if f.is_file()): # Check for files within class dirs\n",
    "             logger.info(f\"Split directories seem populated. Assuming extraction is also complete. Skipping extraction.\")\n",
    "             return \"Skipped - Already Extracted/Split\"\n",
    "\n",
    "        try:\n",
    "            if not local_data_file.exists():\n",
    "                 logger.error(f\"Cannot extract. Zip file '{local_data_file}' does not exist. Run download first.\")\n",
    "                 raise FileNotFoundError(f\"Zip file '{local_data_file}' not found for extraction.\")\n",
    "\n",
    "            logger.info(f\"Extracting '{local_data_file}' into '{unzip_path}'...\")\n",
    "            unzip_path.mkdir(parents=True, exist_ok=True) # Ensure it exists\n",
    "\n",
    "            with zipfile.ZipFile(local_data_file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzip_path)\n",
    "            logger.info(f\"Successfully extracted '{local_data_file}' to '{unzip_path}'\")\n",
    "\n",
    "            extracted_items = list(unzip_path.iterdir())\n",
    "            if not extracted_items:\n",
    "                logger.warning(f\"Extraction finished, but the directory '{unzip_path}' is empty. Check the zip file content and structure.\")\n",
    "            else:\n",
    "                logger.info(f\"Extraction seems successful, found {len(extracted_items)} items in '{unzip_path}'.\")\n",
    "\n",
    "        except zipfile.BadZipFile:\n",
    "             logger.error(f\"Error: '{local_data_file}' is not a valid zip file or is corrupted.\")\n",
    "             raise\n",
    "        except Exception as e:\n",
    "             logger.error(f\"An error occurred during zip extraction: {e}\")\n",
    "             raise e\n",
    "        return \"Extraction Complete\"\n",
    "\n",
    "\n",
    "    # ... (split_data method remains the same) ...\n",
    "    def split_data(self, train_ratio=0.7, test_ratio=0.2): # Removed unused val_ratio param\n",
    "        \"\"\"\n",
    "        Splits the data from unzip_dir into train, test, and validation sets.\n",
    "        Returns a status string.\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data splitting...\")\n",
    "        source_dir = self.config.unzip_dir\n",
    "        train_dir = self.config.train_dir\n",
    "        test_dir = self.config.test_dir\n",
    "        val_dir = self.config.val_dir\n",
    "\n",
    "        # Check if splitting appears complete\n",
    "        if train_dir.exists() and any(f for f in train_dir.glob('*/*') if f.is_file()):\n",
    "            logger.info(\"Train directory already exists and seems populated. Assuming split is already done. Skipping.\")\n",
    "            return \"Skipped - Already Split\"\n",
    "\n",
    "        if not source_dir.exists() or not any(source_dir.iterdir()):\n",
    "             logger.error(f\"Source directory for splitting '{source_dir}' does not exist or is empty. Run extraction first.\")\n",
    "             if self.config.local_data_file.exists():\n",
    "                 logger.error(f\"Zip file '{self.config.local_data_file}' exists, but extraction directory is missing/empty. Try running extraction again.\")\n",
    "             raise FileNotFoundError(f\"Source directory '{source_dir}' not found or empty.\")\n",
    "\n",
    "        # Check ratios\n",
    "        if not (0 < train_ratio < 1 and 0 < test_ratio < 1 and train_ratio + test_ratio < 1):\n",
    "             logger.error(f\"Invalid ratios: train={train_ratio}, test={test_ratio}. They must be between 0 and 1, and sum to less than 1.\")\n",
    "             raise ValueError(\"Invalid train/test ratios for splitting.\")\n",
    "        effective_val_ratio = 1.0 - train_ratio - test_ratio\n",
    "        logger.info(f\"Using split ratios: Train={train_ratio:.2f}, Test={test_ratio:.2f}, Validation={effective_val_ratio:.2f}\")\n",
    "\n",
    "        # --- Start: Corrected Logic for finding data_root_in_zip ---\n",
    "        data_root_in_zip = None\n",
    "        # Explicitly check for the known intermediate directory name(s) first.\n",
    "        # Handle potential case variations observed in logs.\n",
    "        possible_intermediate_names = [\"256_ObjectCategories\", \"256_objectcategories\"]\n",
    "\n",
    "        for name in possible_intermediate_names:\n",
    "            potential_dir = source_dir / name\n",
    "            if potential_dir.is_dir():\n",
    "                # IMPORTANT: Check if this directory actually contains class-like subdirs\n",
    "                # (e.g., directories starting with digits like '001.', '002.')\n",
    "                # This prevents selecting an empty or incorrect intermediate folder.\n",
    "                subdirs = [d for d in potential_dir.iterdir() if d.is_dir() and d.name[:3].isdigit()]\n",
    "                if subdirs:\n",
    "                    logger.info(f\"Found valid intermediate directory '{name}' containing class subdirectories. Using it as data root.\")\n",
    "                    data_root_in_zip = potential_dir\n",
    "                    break # Found the correct one, stop checking\n",
    "\n",
    "        # If no valid intermediate directory was found, check if source_dir itself contains the classes\n",
    "        if data_root_in_zip is None:\n",
    "            logger.warning(f\"Standard intermediate directory not found or not validated. Checking if '{source_dir.name}' contains class directories directly.\")\n",
    "            items_in_source = list(source_dir.iterdir())\n",
    "            # Check for class-like dirs directly in source_dir\n",
    "            class_like_dirs_in_source = [d for d in items_in_source if d.is_dir() and d.name[:3].isdigit()]\n",
    "\n",
    "            # Use a threshold - Caltech256 has > 250 classes.\n",
    "            if len(class_like_dirs_in_source) > 100: # Expecting many classes if this is the root\n",
    "                 logger.info(f\"Found {len(class_like_dirs_in_source)} class-like directories directly in '{source_dir.name}'. Assuming this is the data root.\")\n",
    "                 data_root_in_zip = source_dir\n",
    "            else:\n",
    "                # Log detailed info for debugging before failing\n",
    "                logger.error(f\"Cannot determine data root for splitting.\")\n",
    "                logger.error(f\"Checked for intermediate dirs: {possible_intermediate_names} in {source_dir}\")\n",
    "                logger.error(f\"Checked for direct class dirs (like '###.*') in {source_dir}\")\n",
    "                logger.error(f\"Contents of '{source_dir}': {[item.name for item in source_dir.iterdir()]}\")\n",
    "                # Also log contents of potential intermediate dirs if they exist\n",
    "                for name in possible_intermediate_names:\n",
    "                     potential_dir = source_dir / name\n",
    "                     if potential_dir.is_dir():\n",
    "                         logger.error(f\"Contents of potential intermediate '{name}': {[item.name for item in potential_dir.iterdir()]}\")\n",
    "                return \"Split Failed - Cannot Find Data Root Structure\"\n",
    "        # --- End: Corrected Logic ---\n",
    "\n",
    "\n",
    "        class_dirs = [d for d in data_root_in_zip.iterdir() if d.is_dir()]\n",
    "        # Filter out potential non-class dirs like '.DS_Store' or others if necessary\n",
    "        class_dirs = [d for d in class_dirs if not d.name.startswith('.')]\n",
    "        # Optionally, be more strict:\n",
    "        # class_dirs = [d for d in class_dirs if d.name[:3].isdigit()]\n",
    "\n",
    "        if not class_dirs:\n",
    "            logger.error(f\"No valid class subdirectories (like '001.ak47') found in the determined data root '{data_root_in_zip}'. Cannot perform split.\")\n",
    "            return \"Split Failed - No Class Dirs Found in Root\"\n",
    "\n",
    "        num_classes = len(class_dirs)\n",
    "        logger.info(f\"Found {num_classes} classes in '{data_root_in_zip}'. Starting file distribution.\")\n",
    "\n",
    "        # Ensure split directories exist (CM should have done this, but double-check)\n",
    "        train_dir.mkdir(parents=True, exist_ok=True)\n",
    "        test_dir.mkdir(parents=True, exist_ok=True)\n",
    "        val_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        total_files_copied = 0\n",
    "        # --- Loop through classes and copy files (Logic from previous working version) ---\n",
    "        for i, class_dir in enumerate(class_dirs):\n",
    "            class_name = class_dir.name\n",
    "            # Use debug for potentially verbose per-class logs\n",
    "            logger.debug(f\"Processing class {i+1}/{num_classes}: {class_name} from {class_dir}\")\n",
    "\n",
    "            # List image files within this specific class directory\n",
    "            files = [f for f in class_dir.glob('*') if f.is_file() and not f.name.startswith('.') and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tif', '.tiff']]\n",
    "\n",
    "            if not files:\n",
    "                logger.warning(f\"  No suitable image files found in class directory: {class_dir}. Skipping this class.\")\n",
    "                continue # Skip to the next class\n",
    "\n",
    "            random.shuffle(files)\n",
    "            n_files = len(files)\n",
    "            n_train = int(n_files * train_ratio)\n",
    "            n_test = int(n_files * test_ratio)\n",
    "            # n_val is the remainder\n",
    "            n_val = n_files - n_train - n_test\n",
    "\n",
    "            # Handle edge case: ensure train/test/val get at least one file if possible and n_files > 0\n",
    "            if n_files > 0 and n_train == 0: n_train = 1\n",
    "            if n_files > n_train and n_test == 0: n_test = 1\n",
    "            # Recalculate n_val after potential adjustments\n",
    "            n_val = n_files - n_train - n_test\n",
    "            # Ensure n_val isn't negative if adjustments took needed files\n",
    "            if n_val < 0:\n",
    "                 # This implies n_train + n_test > n_files after adjustment. Prioritize train, then test.\n",
    "                 if n_train + n_test > n_files: n_test = n_files - n_train # Adjust test down first\n",
    "                 if n_test < 0 : n_test = 0 # Ensure test isn't negative\n",
    "                 if n_train > n_files: n_train = n_files # Adjust train if needed (shouldn't happen with test adjust)\n",
    "                 n_val = 0 # Val gets zero if adjustments used all files\n",
    "\n",
    "            logger.debug(f\"  Splitting {n_files} files: Train={n_train}, Test={n_test}, Val={n_val}\")\n",
    "\n",
    "            train_files = files[:n_train]\n",
    "            test_files = files[n_train : n_train + n_test]\n",
    "            val_files = files[n_train + n_test :] # Takes the rest\n",
    "\n",
    "            # Create destination class directories\n",
    "            (train_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "            (test_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "            (val_dir / class_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Copy files\n",
    "            files_copied_this_class = 0\n",
    "            try:\n",
    "                for f_list, dest_dir in [(train_files, train_dir), (test_files, test_dir), (val_files, val_dir)]:\n",
    "                    dest_class_dir = dest_dir / class_name\n",
    "                    for f in f_list:\n",
    "                        try:\n",
    "                            shutil.copy2(str(f), str(dest_class_dir / f.name)) # copy2 preserves metadata\n",
    "                            files_copied_this_class += 1\n",
    "                        except Exception as file_copy_error:\n",
    "                             logger.warning(f\"    Could not copy file {f.name} to {dest_class_dir}: {file_copy_error}\")\n",
    "                             # Continue with other files in the same class/split\n",
    "\n",
    "            except Exception as copy_error:\n",
    "                 logger.error(f\"  Major error during file copying for class {class_name}: {copy_error}\")\n",
    "                 # Depending on severity, you might want to stop or continue\n",
    "                 # raise copy_error # Option: Stop execution\n",
    "                 continue # Option: Continue with the next class\n",
    "\n",
    "            total_files_copied += files_copied_this_class\n",
    "            logger.debug(f\"  Finished class {class_name}, copied {files_copied_this_class} files.\")\n",
    "\n",
    "\n",
    "        if total_files_copied == 0 and num_classes > 0:\n",
    "             logger.error(\"Data splitting loop completed, but 0 files were copied. Check file permissions, disk space, or image file extensions.\")\n",
    "             return \"Split Failed - No Files Copied\"\n",
    "        elif num_classes == 0:\n",
    "             # This case is handled earlier, but double-check\n",
    "             logger.error(\"Data splitting failed because no class directories were processed.\")\n",
    "             return \"Split Failed - No Classes Processed\"\n",
    "        else:\n",
    "            logger.info(f\"Data splitting completed. Copied {total_files_copied} files across {num_classes} classes.\")\n",
    "            return \"Split Complete\"\n",
    "\n",
    "\n",
    "    def cleanup_unzip_dir(self):\n",
    "        \"\"\"\n",
    "        Removes the temporary directory where files were initially unzipped,\n",
    "        if the configuration flag `cleanup_unzip_dir_after_split` is True.\n",
    "        \"\"\"\n",
    "        if not self.config.cleanup_unzip_dir_after_split:\n",
    "            logger.info(\"Cleanup of unzip directory is disabled in the configuration. Skipping removal.\")\n",
    "            return # Exit the function if cleanup is disabled\n",
    "\n",
    "        unzip_path = self.config.unzip_dir\n",
    "        if unzip_path.exists() and unzip_path.is_dir():\n",
    "            logger.info(f\"Attempting to remove temporary extraction directory: {unzip_path}\")\n",
    "            try:\n",
    "                shutil.rmtree(unzip_path)\n",
    "                logger.info(f\"Successfully removed directory: {unzip_path}\")\n",
    "            except OSError as e:\n",
    "                # OSError is common for permission errors or if dir is in use\n",
    "                logger.error(f\"Error removing directory {unzip_path}: {e}. Check permissions or if it's in use.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"An unexpected error occurred while removing directory {unzip_path}: {e}\")\n",
    "        else:\n",
    "            logger.warning(f\"Directory {unzip_path} not found or is not a directory. Skipping cleanup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-03 15:23:43,675: INFO: utils: yaml file config/config.yaml loaded successfully:]\n",
      "[2025-06-03 15:23:43,679: INFO: utils: yaml file config/params.yaml loaded successfully:]\n",
      "[2025-06-03 15:23:43,681: INFO: utils: created directory at: artifacts:]\n",
      "[2025-06-03 15:23:43,682: INFO: utils: created directory at: artifacts/data_ingestion:]\n",
      "[2025-06-03 15:23:43,683: INFO: utils: created directory at: artifacts/data_ingestion/extracted_data:]\n",
      "[2025-06-03 15:23:43,684: INFO: utils: created directory at: artifacts/data_ingestion/train:]\n",
      "[2025-06-03 15:23:43,685: INFO: utils: created directory at: artifacts/data_ingestion/test:]\n",
      "[2025-06-03 15:23:43,686: INFO: utils: created directory at: artifacts/data_ingestion/val:]\n",
      "[2025-06-03 15:23:43,687: INFO: 645679405: --- Starting Data Ingestion Pipeline ---:]\n",
      "[2025-06-03 15:23:43,687: INFO: 2359565039: Downloading dataset 'jessicali9530/caltech256' from Kaggle into 'artifacts/data_ingestion'...:]\n",
      "[2025-06-03 15:26:32,263: INFO: 2359565039: Successfully downloaded 'jessicali9530/caltech256' to 'artifacts/data_ingestion/caltech256.zip':]\n",
      "[2025-06-03 15:26:32,842: INFO: 645679405: Download Status: Download Complete:]\n",
      "[2025-06-03 15:26:33,046: INFO: 2359565039: Extracting 'artifacts/data_ingestion/caltech256.zip' into 'artifacts/data_ingestion/extracted_data'...:]\n",
      "[2025-06-03 15:26:50,279: INFO: 2359565039: Successfully extracted 'artifacts/data_ingestion/caltech256.zip' to 'artifacts/data_ingestion/extracted_data':]\n",
      "[2025-06-03 15:26:50,280: INFO: 2359565039: Extraction seems successful, found 2 items in 'artifacts/data_ingestion/extracted_data'.:]\n",
      "[2025-06-03 15:26:50,295: INFO: 645679405: Extraction Status: Extraction Complete:]\n",
      "[2025-06-03 15:26:50,299: INFO: 2359565039: Starting data splitting...:]\n",
      "[2025-06-03 15:26:50,303: INFO: 2359565039: Using split ratios: Train=0.70, Test=0.20, Validation=0.10:]\n",
      "[2025-06-03 15:26:51,314: INFO: 2359565039: Found valid intermediate directory '256_ObjectCategories' containing class subdirectories. Using it as data root.:]\n",
      "[2025-06-03 15:26:51,317: INFO: 2359565039: Found 257 classes in 'artifacts/data_ingestion/extracted_data/256_ObjectCategories'. Starting file distribution.:]\n",
      "[2025-06-03 15:27:09,354: INFO: 2359565039: Data splitting completed. Copied 30607 files across 257 classes.:]\n",
      "[2025-06-03 15:27:09,356: INFO: 645679405: Split Status: Split Complete:]\n",
      "[2025-06-03 15:27:09,357: INFO: 645679405: Splitting is complete or was already done. Proceeding to cleanup check.:]\n",
      "[2025-06-03 15:27:09,357: INFO: 2359565039: Attempting to remove temporary extraction directory: artifacts/data_ingestion/extracted_data:]\n",
      "[2025-06-03 15:27:11,289: INFO: 2359565039: Successfully removed directory: artifacts/data_ingestion/extracted_data:]\n",
      "[2025-06-03 15:27:11,290: INFO: 645679405: --- Data Ingestion Pipeline Finished ---:]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        config_manager = ConfigurationManager()\n",
    "        data_ingestion_config = config_manager.get_data_ingestion_config()\n",
    "        data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "\n",
    "        logger.info(\"--- Starting Data Ingestion Pipeline ---\")\n",
    "\n",
    "        download_status = data_ingestion.download_file()\n",
    "        logger.info(f\"Download Status: {download_status}\")\n",
    "\n",
    "        extract_status = \"Not Run\" # Initialize status\n",
    "        split_status = \"Not Run\"   # Initialize status\n",
    "\n",
    "        if download_status != \"Skipped - Already Processed\":\n",
    "            extract_status = data_ingestion.extract_zip_file()\n",
    "            logger.info(f\"Extraction Status: {extract_status}\")\n",
    "\n",
    "            # Proceed to split only if extraction was successful or skipped because it existed\n",
    "            # (It shouldn't be skipped if download was just completed)\n",
    "            if extract_status in [\"Extraction Complete\", \"Skipped - Already Extracted/Split\", \"Extraction Skipped - Exists\"]: # Added hypothetical status\n",
    "                 split_status = data_ingestion.split_data(train_ratio=0.7, test_ratio=0.2)\n",
    "                 logger.info(f\"Split Status: {split_status}\")\n",
    "            else:\n",
    "                 logger.warning(f\"Skipping split step because extraction status was: {extract_status}\")\n",
    "\n",
    "        else:\n",
    "            # If download was skipped because everything seemed done, let's confirm split status for cleanup\n",
    "            logger.info(\"Download skipped as data seems processed. Checking split status for potential cleanup.\")\n",
    "            split_status = \"Skipped - Already Split\" # Assume split is also done if download skipped\n",
    "\n",
    "        if split_status in [\"Split Complete\", \"Skipped - Already Split\"]:\n",
    "            logger.info(\"Splitting is complete or was already done. Proceeding to cleanup check.\")\n",
    "            data_ingestion.cleanup_unzip_dir() # The method itself checks the config flag\n",
    "        else:\n",
    "            logger.warning(f\"Skipping cleanup because split status was: {split_status}. The raw extracted data will be kept in {data_ingestion_config.unzip_dir}\")\n",
    "\n",
    "        logger.info(\"--- Data Ingestion Pipeline Finished ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(\"An error occurred during the data ingestion pipeline:\")\n",
    "        # Optional: Re-raise to halt execution on error\n",
    "        # raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
