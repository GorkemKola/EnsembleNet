{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68de443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gorkemkola/Desktop/Projects/cv_project\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d901f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ensemblenet import logger\n",
    "from ensemblenet.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from ensemblenet.utils import read_yaml, create_directories\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f596135",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PlotConfig:\n",
    "    root_dir: Path\n",
    "    model_names: List[str]\n",
    "    params: Dict[str, any]\n",
    "    training_metrics: List[pd.DataFrame]\n",
    "    figsize: List[int]\n",
    "    color_palette: str\n",
    "    sns_style: str\n",
    "    test_results: pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10a115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "    ) -> None:\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        \n",
    "        create_directories(\n",
    "            [\n",
    "                self.config.artifacts_root,\n",
    "                Path(self.config.plot.root_dir),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_plot_config(self) -> PlotConfig:\n",
    "        model_names = sorted(self.config.test.model_names)\n",
    "        training_metric_paths = [self.config.plot.training_metrics_path_template.format(model_name=name) for name in model_names]\n",
    "        training_metrics = [pd.read_csv(path) for path in training_metric_paths]\n",
    "        test_results_path = self.config.plot.test_results_path\n",
    "        test_results = pd.read_csv(test_results_path) if Path(test_results_path).exists() else pd.DataFrame()\n",
    "        plot_config = PlotConfig(\n",
    "            root_dir=Path(self.config.plot.root_dir),\n",
    "            model_names=model_names,\n",
    "            params=self.params,\n",
    "            training_metrics=training_metrics,\n",
    "            test_results=test_results,\n",
    "            figsize=self.params.FIGSIZE,\n",
    "            color_palette=self.params.COLOR_PALETTE,\n",
    "            sns_style=self.params.SNS_STYLE,\n",
    "        )\n",
    "        \n",
    "        return plot_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c8fc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 11:29:41,064: INFO: utils: yaml file config/config.yaml loaded successfully:]\n",
      "[2025-06-21 11:29:41,068: INFO: utils: yaml file config/params.yaml loaded successfully:]\n",
      "[2025-06-21 11:29:41,069: INFO: utils: created directory at: artifacts:]\n",
      "[2025-06-21 11:29:41,070: INFO: utils: created directory at: artifacts/plot:]\n"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager().get_plot_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa01ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(self, config: PlotConfig) -> None:\n",
    "        self.config = config\n",
    "        self.figsize = config.figsize\n",
    "        plt.style.use('default')\n",
    "        sns.set_style(config.sns_style)\n",
    "        self.colors = sns.color_palette(config.color_palette, 10)\n",
    "\n",
    "   \n",
    "        \n",
    "    def plot_all_metrics(self, save_path: str = None, show_plot: bool = True):\n",
    "        \"\"\"\n",
    "        Plot all training metrics for multiple models.\n",
    "        \n",
    "        Args:\n",
    "            model_names: List of model names\n",
    "            dataframes: List of corresponding pandas DataFrames with training metrics\n",
    "            save_path: Optional path to save the plot\n",
    "            show_plot: Whether to display the plot\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=self.figsize)\n",
    "        fig.suptitle('Training Metrics Comparison Across Models', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Define metrics to plot\n",
    "        metrics = [\n",
    "            ('train_loss', 'Training Loss', axes[0, 0]),\n",
    "            ('valid_loss', 'Validation Loss', axes[0, 1]),\n",
    "            ('accuracy', 'Accuracy (%)', axes[0, 2]),\n",
    "            ('precision', 'Precision', axes[1, 0]),\n",
    "            ('recall', 'Recall', axes[1, 1]),\n",
    "            ('f1_score', 'F1 Score', axes[1, 2])\n",
    "        ]\n",
    "        \n",
    "        # Plot each metric\n",
    "        for metric, title, ax in metrics:\n",
    "            self._plot_metric(metric, title, ax)\n",
    "        \n",
    "        # Add learning rate plot as an inset or separate info\n",
    "        self._add_learning_rate_info(fig)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _plot_metric(self,\n",
    "                    metric: str, title: str, ax):\n",
    "        \"\"\"Plot a single metric for all models.\"\"\"\n",
    "        model_names = self.config.model_names\n",
    "        dataframes = self.config.training_metrics\n",
    "        \n",
    "        for i, (name, df) in enumerate(zip(model_names, dataframes)):\n",
    "            if metric in df.columns:\n",
    "                epochs = df['epoch'] + 1  # Start from epoch 1\n",
    "                values = df[metric]\n",
    "                \n",
    "                # Use different colors for each model\n",
    "                color = self.colors[i % len(self.colors)]\n",
    "                ax.plot(epochs, values, marker='o', linewidth=2, \n",
    "                       markersize=4, label=name, color=color, alpha=0.8)\n",
    "                \n",
    "                # Highlight best model epochs\n",
    "                if 'is_best_model' in df.columns:\n",
    "                    best_epochs = df[df['is_best_model'] == True]\n",
    "                    if not best_epochs.empty:\n",
    "                        best_x = best_epochs['epoch'] + 1\n",
    "                        best_y = best_epochs[metric]\n",
    "                        ax.scatter(best_x, best_y, color=color, s=100, \n",
    "                                 marker='*', edgecolors='black', linewidth=1,\n",
    "                                 zorder=5)\n",
    "        \n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=8)\n",
    "        \n",
    "        # Format y-axis based on metric type\n",
    "        if metric == 'accuracy':\n",
    "            ax.set_ylim(0, 100)\n",
    "        elif metric in ['precision', 'recall', 'f1_score']:\n",
    "            ax.set_ylim(0, 1)\n",
    "\n",
    "    def _add_learning_rate_info(self, fig):\n",
    "        \"\"\"Add learning rate information to the plot.\"\"\"\n",
    "        model_names = self.config.model_names\n",
    "        dataframes = self.config.training_metrics\n",
    "        \n",
    "        lr_info = []\n",
    "        for name, df in zip(model_names, dataframes):\n",
    "            if 'learning_rate' in df.columns:\n",
    "                initial_lr = df['learning_rate'].iloc[0]\n",
    "                final_lr = df['learning_rate'].iloc[-1]\n",
    "                lr_changes = len(df['learning_rate'].unique())\n",
    "                lr_info.append(f\"{name}: {initial_lr:.6f} â†’ {final_lr:.6f} ({lr_changes} changes)\")\n",
    "        \n",
    "        if lr_info:\n",
    "            fig.text(0.02, 0.02, \"Learning Rate Info:\\n\" + \"\\n\".join(lr_info), \n",
    "                    fontsize=8, verticalalignment='bottom',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.5))\n",
    "            \n",
    "        \n",
    "    def plot_loss_comparison(self,\n",
    "                           save_path: str = None, show_plot: bool = True):\n",
    "        \"\"\"\n",
    "        Create a focused comparison of training and validation losses.\n",
    "        \"\"\"\n",
    "        model_names = self.config.model_names\n",
    "        dataframes = self.config.training_metrics\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        fig.suptitle('Loss Comparison Across Models', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for i, (name, df) in enumerate(zip(model_names, dataframes)):\n",
    "            epochs = df['epoch'] + 1\n",
    "            color = self.colors[i % len(self.colors)]\n",
    "            \n",
    "            # Training loss\n",
    "            ax1.plot(epochs, df['train_loss'], marker='o', linewidth=2, \n",
    "                    markersize=3, label=name, color=color, alpha=0.8)\n",
    "            \n",
    "            # Validation loss\n",
    "            ax2.plot(epochs, df['valid_loss'], marker='s', linewidth=2, \n",
    "                    markersize=3, label=name, color=color, alpha=0.8)\n",
    "        \n",
    "        ax1.set_title('Training Loss', fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.set_title('Validation Loss', fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        \n",
    "        return fig\n",
    "    def plot_accuracy_comparison(self,\n",
    "                                save_path: str = None, show_plot: bool = True):\n",
    "        \"\"\"\n",
    "        Create a focused comparison of accuracy metrics.\n",
    "        \"\"\"\n",
    "        model_names = self.config.model_names\n",
    "        dataframes = self.config.training_metrics\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        fig.suptitle('Accuracy Comparison Across Models', fontsize=14, fontweight='bold')\n",
    "        for i, (name, df) in enumerate(zip(model_names, dataframes)):\n",
    "            epochs = df['epoch'] + 1\n",
    "            color = self.colors[i % len(self.colors)]\n",
    "            \n",
    "            ax.plot(epochs, df['accuracy'], marker='o', linewidth=2, \n",
    "                    markersize=3, label=name, color=color, alpha=0.8)\n",
    "            \n",
    "            # Highlight best model epochs\n",
    "            if 'is_best_model' in df.columns:\n",
    "                best_epochs = df[df['is_best_model'] == True]\n",
    "                if not best_epochs.empty:\n",
    "                    best_x = best_epochs['epoch'] + 1\n",
    "                    best_y = best_epochs['accuracy']\n",
    "                    ax.scatter(best_x, best_y, color=color, s=100, \n",
    "                             marker='*', edgecolors='black', linewidth=1,\n",
    "                             zorder=5)\n",
    "        ax.set_title('Accuracy', fontweight='bold')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        return fig\n",
    "    def plot_precision_recall_f1(self,\n",
    "                                 save_path: str = None, show_plot: bool = True):\n",
    "        \"\"\"\n",
    "        Create a focused comparison of precision, recall, and F1 score.\n",
    "        \"\"\"\n",
    "        model_names = self.config.model_names\n",
    "        dataframes = self.config.training_metrics\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle('Precision, Recall, and F1 Score Comparison Across Models', fontsize=14, fontweight='bold')\n",
    "        metrics = ['precision', 'recall', 'f1_score']\n",
    "        titles = ['Precision', 'Recall', 'F1 Score']\n",
    "        for i, (metric, title, ax) in enumerate(zip(metrics, titles, axes)):\n",
    "            for j, (name, df) in enumerate(zip(model_names, dataframes)):\n",
    "                epochs = df['epoch'] + 1\n",
    "                color = self.colors[j % len(self.colors)]\n",
    "                \n",
    "                ax.plot(epochs, df[metric], marker='o', linewidth=2, \n",
    "                        markersize=3, label=name, color=color, alpha=0.8)\n",
    "                \n",
    "                # Highlight best model epochs\n",
    "                if 'is_best_model' in df.columns:\n",
    "                    best_epochs = df[df['is_best_model'] == True]\n",
    "                    if not best_epochs.empty:\n",
    "                        best_x = best_epochs['epoch'] + 1\n",
    "                        best_y = best_epochs[metric]\n",
    "                        ax.scatter(best_x, best_y, color=color, s=100, \n",
    "                                 marker='*', edgecolors='black', linewidth=1,\n",
    "                                 zorder=5)\n",
    "            ax.set_title(title, fontweight='bold')\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel(title)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        return fig\n",
    "    def plot_learning_rate(self,\n",
    "                           save_path: str = None, show_plot: bool = True):\n",
    "        \"\"\"\n",
    "        Create a focused plot of learning rate changes.\n",
    "        \"\"\"\n",
    "\n",
    "        model_names = self.config.model_names\n",
    "        dataframes = self.config.training_metrics\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        fig.suptitle('Learning Rate Changes Across Models', fontsize=14, fontweight='bold')\n",
    "        for i, (name, df) in enumerate(zip(model_names, dataframes)):\n",
    "            if 'learning_rate' in df.columns:\n",
    "                epochs = df['epoch'] + 1\n",
    "                color = self.colors[i % len(self.colors)]\n",
    "                \n",
    "                ax.plot(epochs, df['learning_rate'], marker='o', linewidth=2, \n",
    "                        markersize=3, label=name, color=color, alpha=0.8)\n",
    "                # Highlight best model epochs\n",
    "                if 'is_best_model' in df.columns:\n",
    "                    best_epochs = df[df['is_best_model'] == True]\n",
    "                    if not best_epochs.empty:\n",
    "                        best_x = best_epochs['epoch'] + 1\n",
    "                        best_y = best_epochs['learning_rate']\n",
    "                        ax.scatter(best_x, best_y, color=color, s=100, \n",
    "                                 marker='*', edgecolors='black', linewidth=1,\n",
    "                                 zorder=5)\n",
    "        ax.set_title('Learning Rate', fontweight='bold')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Learning Rate')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        if show_plot:\n",
    "            plt.show()\n",
    "        return fig\n",
    "    \n",
    "    def compute_dataset_mean_std(self, data_dir, image_size=(224, 224), batch_size=64, sample_limit=None):\n",
    "        \"\"\"\n",
    "        Computes mean and std for all images in a directory (ImageFolder structure).\n",
    "        Uses cached .npy arrays if available, otherwise computes and saves them.\n",
    "        \"\"\"\n",
    "        mean_path = Path(data_dir) / \"mean.npy\"\n",
    "        std_path = Path(data_dir) / \"std.npy\"\n",
    "\n",
    "        if mean_path.exists() and std_path.exists():\n",
    "            logger.info(f\"Loading cached mean/std from {mean_path} and {std_path}\")\n",
    "            mean = np.load(mean_path)\n",
    "            std = np.load(std_path)\n",
    "            return mean.tolist(), std.tolist()\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        n_images = 0\n",
    "        mean = 0.\n",
    "        std = 0.\n",
    "        for i, (imgs, _) in enumerate(loader):\n",
    "            if sample_limit and n_images >= sample_limit:\n",
    "                break\n",
    "            imgs = imgs.view(imgs.size(0), imgs.size(1), -1)\n",
    "            mean += imgs.mean(2).sum(0)\n",
    "            std += imgs.std(2).sum(0)\n",
    "            n_images += imgs.size(0)\n",
    "            if sample_limit and n_images >= sample_limit:\n",
    "                break\n",
    "\n",
    "        mean /= n_images\n",
    "        std /= n_images\n",
    "\n",
    "        np.save(mean_path, mean.cpu().numpy() if hasattr(mean, \"cpu\") else mean)\n",
    "        np.save(std_path, std.cpu().numpy() if hasattr(std, \"cpu\") else std)\n",
    "        logger.info(f\"Saved computed mean/std to {mean_path} and {std_path}\")\n",
    "\n",
    "        return mean.tolist(), std.tolist()\n",
    "    \n",
    "        \n",
    "    def plot_test_results(self, save_path: str = None, columns: List[str] = None, show_plot: bool = True):\n",
    "        \"\"\"\n",
    "        Plot each test metric in a separate bar chart.\n",
    "        Args:\n",
    "            save_path: Optional directory to save the plots (each metric will be saved as a separate file)\n",
    "            columns: List of columns to plot (default is all except 'model')\n",
    "            show_plot: Whether to display the plot\n",
    "        \"\"\"\n",
    "        results_df = self.config.test_results.copy()\n",
    "        if columns is None:\n",
    "            columns = results_df.columns.tolist()\n",
    "            columns.remove('model')\n",
    "        for metric in columns:\n",
    "            fig, ax = plt.subplots(figsize=self.config.figsize)\n",
    "            colors = sns.color_palette(self.config.color_palette, n_colors=len(results_df))\n",
    "            results_df.set_index('model')[metric].plot(kind='bar', ax=ax, color=colors, alpha=0.8)\n",
    "            ax.set_title(f'{metric.capitalize()} Comparison Across Models', fontsize=16, fontweight='bold')\n",
    "            ax.set_ylabel(metric.capitalize())\n",
    "            ax.set_xlabel('Model')\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            if save_path:\n",
    "                # Save each metric plot as a separate file\n",
    "                plt.savefig(os.path.join(save_path, f\"test_{metric}_barchart.png\"), dpi=300, bbox_inches='tight')\n",
    "            if show_plot:\n",
    "                plt.show()\n",
    "            plt.close(fig)\n",
    "        return\n",
    "\n",
    "    def get_number_of_params(self, model_name) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Get the number of parameters in a model.\n",
    "        \n",
    "        Args:\n",
    "            model: The model instance\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary with the number of parameters\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        model = torch.load()\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        return {\"num_params\": num_params}\n",
    "    \n",
    "    def extract_model_params(self, save_path: str = None, show_plot: bool = True):\n",
    "        \"\"\"\n",
    "        Plot the number of parameters for each model.\n",
    "        \n",
    "        Args:\n",
    "            save_path: Optional path to save the plot\n",
    "            show_plot: Whether to display the plot\n",
    "        \"\"\"\n",
    "        num_params = [self.get_number_of_params(model).get(\"num_params\") for model_name in self.config.model_paths]\n",
    "        \n",
    "        # save to csv\n",
    "        params_df = pd.DataFrame({\n",
    "            'model': self.config.model_names,\n",
    "            'num_params': num_params\n",
    "        })\n",
    "        params_csv_path = self.config.root_dir / \"model_params.csv\"\n",
    "        params_df.to_csv(params_csv_path, index=False)\n",
    "        logger.info(f\"Saved model parameters to {params_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e7e01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 11:32:35,622: INFO: utils: yaml file config/config.yaml loaded successfully:]\n",
      "[2025-06-21 11:32:35,626: INFO: utils: yaml file config/params.yaml loaded successfully:]\n",
      "[2025-06-21 11:32:35,627: INFO: utils: created directory at: artifacts:]\n",
      "[2025-06-21 11:32:35,629: INFO: utils: created directory at: artifacts/plot:]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Plotter' object has no attribute 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m config = ConfigurationManager().get_plot_config()\n\u001b[32m      2\u001b[39m plotter = Plotter(config)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mplotter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_model_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m plotter.plot_test_results(save_path=config.root_dir, columns=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      5\u001b[39m plotter.plot_all_metrics(save_path=config.root_dir / \u001b[33m\"\u001b[39m\u001b[33mall_metrics_comparison.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 363\u001b[39m, in \u001b[36mPlotter.extract_model_params\u001b[39m\u001b[34m(self, save_path, show_plot)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_model_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, save_path: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m, show_plot: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    356\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[33;03m    Plot the number of parameters for each model.\u001b[39;00m\n\u001b[32m    358\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    361\u001b[39m \u001b[33;03m        show_plot: Whether to display the plot\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     num_params = [\u001b[38;5;28mself\u001b[39m.get_number_of_params(model).get(\u001b[33m\"\u001b[39m\u001b[33mnum_params\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m]\n\u001b[32m    365\u001b[39m     \u001b[38;5;66;03m# save to csv\u001b[39;00m\n\u001b[32m    366\u001b[39m     params_df = pd.DataFrame({\n\u001b[32m    367\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mself\u001b[39m.config.model_names,\n\u001b[32m    368\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mnum_params\u001b[39m\u001b[33m'\u001b[39m: num_params\n\u001b[32m    369\u001b[39m     })\n",
      "\u001b[31mAttributeError\u001b[39m: 'Plotter' object has no attribute 'models'"
     ]
    }
   ],
   "source": [
    "config = ConfigurationManager().get_plot_config()\n",
    "plotter = Plotter(config)\n",
    "plotter.extract_model_params(save_path=config.root_dir, show_plot=False)\n",
    "plotter.plot_test_results(save_path=config.root_dir, columns=['accuracy', 'precision', 'recall'])\n",
    "plotter.plot_all_metrics(save_path=config.root_dir / \"all_metrics_comparison.png\")\n",
    "plotter.plot_loss_comparison(save_path=config.root_dir / \"loss_comparison.png\")\n",
    "plotter.plot_accuracy_comparison(save_path=config.root_dir / \"accuracy_comparison.png\")\n",
    "plotter.plot_precision_recall_f1(save_path=config.root_dir / \"precision_recall_f1_comparison.png\")\n",
    "plotter.plot_learning_rate(save_path=config.root_dir / \"learning_rate_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592b0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
